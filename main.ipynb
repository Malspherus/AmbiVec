{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% Imports\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import logging\n",
    "from rdflib import Graph, Namespace, Literal\n",
    "from rdflib.plugins.parsers import trig\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from rdflib import URIRef\n",
    "from rdflib.namespace import RDF\n",
    "from rdflib.namespace import RDFS\n",
    "from rdflib.namespace import SKOS\n",
    "from tqdm import tqdm\n",
    "from TqdmToLogger import TqdmToLogger\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "tqdm.pandas()\n",
    "tqdm_out = TqdmToLogger(logger, level=logging.INFO)\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', None)\n",
    "#pd.reset_option('max_columns')\n",
    "#pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% Load/train dictionary\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gw1 = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wv = gw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% Load/train dictionary\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gw3 = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "# wk = api.load('wiki-english-20171001')\n",
    "\n",
    "# Wiki has to be converted, but JSON is not supported...\n",
    "# wiki = gensim.corpora.WikiCorpus(wk)\n",
    "\n",
    "# Train and save\n",
    "# model = gensim.models.Word2Vec(wk, workers=4)\n",
    "# 14774682 raw words (853473 effective words) took 610.2s with 3 workers\n",
    "\n",
    "# with tempfile.NamedTemporaryFile(prefix='wiki-english-20171001-model-', delete=False) as tmp:\n",
    "#    temporary_filepath = tmp.name\n",
    "#    model.save(temporary_filepath)\n",
    "\n",
    "# Load\n",
    "# model = gensim.models.Word2Vec.load(temporary_filepath)\n",
    "# wv = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% Load KG\n"
    }
   },
   "outputs": [],
   "source": [
    "kb = Graph()\n",
    "kb.parse(\"C:/Users/Peter/gensim-data/KBpedia/kbpedia_reference_concepts.n3\", format=\"n3\")\n",
    "logger.info(f\"Loaded graph with {len(kb)} triples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBpedia disjointDomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Graph()\n",
    "db.parse(\"C:/Users/Peter/gensim-data/DBpedia/mappingbased-objects_lang=en_disjointDomain.ttl\", format=\"ttl\")\n",
    "logger.info(f\"Loaded graph with {len(db)} triples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBpedia disjointRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = Graph()\n",
    "db2.parse(\"C:/Users/Peter/gensim-data/DBpedia/mappingbased-objects_lang=en_disjointRange.ttl\", format=\"ttl\")\n",
    "logger.info(f\"Loaded graph with {len(db2)} triples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg = Graph()\n",
    "tg.parse(\"KGDemo.ttl\", format=\"ttl\")\n",
    "logger.info(f\"Loaded graph with {len(tg)} triples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pokemon KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = Graph()\n",
    "pg.parse(\"pokemon.nq\", format=\"nquads\")\n",
    "logger.info(f\"Loaded graph with {len(pg)} triples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Webster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg = Graph()\n",
    "wg.parse(\"WBT_DSR_LC_model_XML.rdf\")\n",
    "logger.info(f\"Loaded graph with {len(pg)} triples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = wg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beatles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = Graph()\n",
    "bg.parse(\"beatles.ttl\", format=\"ttl\")\n",
    "logger.info(f\"Loaded graph with {len(bg)} triples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = bg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dg = Graph()\n",
    "dg.parse(\"diet.ttl\", format=\"ttl\")\n",
    "logger.info(f\"Loaded graph with {len(dg)} triples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "g = dg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ig = Graph()\n",
    "ig.parse(\"ingredients.ttl\", format=\"ttl\")\n",
    "logger.info(f\"Loaded graph with {len(ig)} triples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "g = ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sparql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sg = Graph()\n",
    "sg.parse(\"sparql\", format=\"ttl\")\n",
    "logger.info(f\"Loaded graph with {len(sg)} triples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "g = sg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getPreferredTitle(n, lang=\"en\"):\n",
    "    label = g.preferredLabel(n, lang=lang)\n",
    "\n",
    "    #if type(n) is not type(Literal(\"\")): #only labels should be of type literal\n",
    "    if label == []:\n",
    "        return n.rsplit('/', 1)[-1].replace('_', ' ').replace(',', '').lower() #TODO: replace \"()\"?\n",
    "    else:\n",
    "        return label[0][1].value.lower()\n",
    "    #else:\n",
    "    #    return None\n",
    "\n",
    "#Returns [vec, isMultipart, multipart-matched-%]\n",
    "def toVector(n):\n",
    "    title = getPreferredTitle(n)\n",
    "    #if title is None:\n",
    "    #    return [None, None, None]\n",
    "    \n",
    "    #In case of multiple words in title use mean of individual vectors\n",
    "    if \" \" in title:\n",
    "        subvecs = []\n",
    "        count = 0\n",
    "        hit = 0\n",
    "        for word in title.split(\" \"):\n",
    "            count += 1\n",
    "            try:\n",
    "                subvecs += [wv[word]]\n",
    "                hit += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "        if hit > 0:\n",
    "            return [sum(subvecs)/hit, True, hit/count]\n",
    "        else:\n",
    "            return [None, True, 0]\n",
    "    else:\n",
    "        try:\n",
    "            return [wv[title], False, False]\n",
    "        except KeyError:\n",
    "            return [None, False, False]\n",
    "\n",
    "#Methods to ambiguify nodes and relations\n",
    "def select(inp, obj={'perc': None, 'num': None}):\n",
    "    if len(obj) > 1:\n",
    "        raise TypeError(\"Please give exactly one of percentage or number\")\n",
    "        \n",
    "    for val in obj:\n",
    "        if val == 'perc':\n",
    "            return inp.sample(frac=obj[val], replace=True)\n",
    "        else:\n",
    "            return inp.sample(n=obj[val], replace=True)\n",
    "\n",
    "def ambiguify(config, nodeVectors, relationVectors):\n",
    "    out = pd.DataFrame()\n",
    "    for target in config: #can be \"nodes\" or \"relations\"\n",
    "        for method in config[target]: #matches the name of the method\n",
    "            for instance in config[target][method]: #once for every instance of the method config\n",
    "                for val in instance['amount']: #the amount of elements to be changed\n",
    "                    logger.info(f\"Ambiguifying {target} with {method} (parameters: {instance})\")\n",
    "                    if target == 'nodes':\n",
    "                        inp = nodeVectors\n",
    "                    else:\n",
    "                        inp = relationVectors\n",
    "                    \n",
    "                    selres = select(inp, instance['amount'])\n",
    "                    conf = instance.get('param', None)\n",
    "                    sourceColumn = target[0:-1]\n",
    "                    rep = pd.DataFrame()\n",
    "                    \n",
    "                    rep[['method', 'config', 'source_type', 'source', 'target']] = selres.progress_apply(lambda sel: pd.Series([\n",
    "                        method,\n",
    "                        str(instance),\n",
    "                        sourceColumn,\n",
    "                        sel[sourceColumn],\n",
    "                        methods[target][method](sel.copy(), inp.copy(), conf)[sourceColumn].iloc[0]\n",
    "                    ]), axis=1)\n",
    "                    \n",
    "                    out = out.append(rep, ignore_index=True)\n",
    "    return out\n",
    "\n",
    "#Modify triple and save as new\n",
    "def modTriple(row, g2, useObject = False, retry=False):\n",
    "    if row['source_type'] == 'relation':\n",
    "        logger.debug(\"Replacing r\")\n",
    "        fil = fullVectors['p'] == row['source']\n",
    "    else:\n",
    "        if useObject:\n",
    "            logger.debug(\"Replacing o\")\n",
    "            fil = fullVectors['o'] == row['source']\n",
    "        else:\n",
    "            logger.debug(\"Replacing s\")\n",
    "            fil = fullVectors['s'] == row['source']\n",
    "    \n",
    "    res = fullVectors[fil]\n",
    "\n",
    "    if len(res) == 0:\n",
    "        if not retry:\n",
    "            return modTriple(row, g2, useObject= not useObject, retry=True)\n",
    "        else:\n",
    "            logger.error(f\"Could not find original triple!\")\n",
    "    else:\n",
    "        rep = res.sample(n=1)\n",
    "\n",
    "        #add row to graph with changed content\n",
    "        if row['source_type'] == 'relation':\n",
    "            g2.add((rep['s'].iloc[0], row['target'], rep['o'].iloc[0]))\n",
    "            logger.debug(f\"{(rep['s'].iloc[0], row['target'], rep['o'].iloc[0])}\")\n",
    "            return (rep['s'].iloc[0], row['target'], rep['o'].iloc[0], rep['s'].iloc[0], rep['p'].iloc[0], rep['o'].iloc[0])\n",
    "        else:\n",
    "            if useObject:\n",
    "                g2.add((rep['s'].iloc[0], rep['p'].iloc[0], row['target']))\n",
    "                logger.debug(f\"{(rep['s'].iloc[0], rep['p'].iloc[0], row['target'])}\")\n",
    "                return (rep['s'].iloc[0], rep['p'].iloc[0], row['target'], rep['s'].iloc[0], rep['p'].iloc[0], rep['o'].iloc[0])\n",
    "            else:\n",
    "                g2.add((row['target'], rep['p'].iloc[0], rep['o'].iloc[0]))\n",
    "                logger.debug(f\"{(row['target'], rep['p'].iloc[0], rep['o'].iloc[0])}\")\n",
    "                return (row['target'], rep['p'].iloc[0], rep['o'].iloc[0], rep['s'].iloc[0], rep['p'].iloc[0], rep['o'].iloc[0])\n",
    "        \n",
    "def populateAdditions(res, g2):\n",
    "    logger.info(f\"Populatig graph\")\n",
    "    \n",
    "    out = pd.DataFrame()\n",
    "    out[['s', 'p', 'o', 's_orig', 'p_orig', 'o_orig']] = res.progress_apply(lambda row: pd.Series(modTriple(row, g2, useObject=(np.random.random() >= 0.5))), axis=1)\n",
    "    \n",
    "    res[['s_orig', 'p_orig', 'o_orig']] = out[['s_orig', 'p_orig', 'o_orig']]\n",
    "    out = out.drop(columns={'s_orig', 'p_orig', 'o_orig'})\n",
    "    \n",
    "    return out\n",
    "\n",
    "#Returns percent of ambiguity in the graph\n",
    "def calculateAmbiguity(fullVectors, nodeVectors, relationVectors):\n",
    "    #for relations\n",
    "    weights = relationVectors['total'] - relationVectors['lost'] - relationVectors['zero_vector']\n",
    "    relAmbig = np.average(relationVectors['mean_dist'], weights=weights)\n",
    "    \n",
    "    #for nodes\n",
    "    weights = nodeVectors['total']\n",
    "    nodeAmbig = np.average(nodeVectors['est_dist'], weights=weights)\n",
    "    \n",
    "    #mean of ambiguities of nodes and vectors, weight by nodes 2:1 relation\n",
    "    totAmbig = np.average([relAmbig, nodeAmbig], weights=[2, 1])\n",
    "    \n",
    "    #transform -1..1 where 1 is the least ambiguous to 0..1 where 1 is the most ambiguous\n",
    "    return 1-((1+totAmbig)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Setup methods\n",
    "def rand(inp, source, conf):\n",
    "    return source.sample(n=1)\n",
    "\n",
    "#Find result with specific distance\n",
    "def dist(inp, source, conf):\n",
    "    if conf == None:\n",
    "        dist = 1\n",
    "    else:\n",
    "        dist = conf.get('dist', 1)\n",
    "        \n",
    "    dist = min(max(dist, 0), len(source.index)-1)\n",
    "    \n",
    "    #use pandas to get top-n, if dist is same move inp to the top\n",
    "    source[['dist', 'isInp']] = source.apply(lambda row: pd.Series([wv.cosine_similarities(list(inp['vec']), [list(row['vec'])])[0], inp[0] == row[0]]), axis = 1)\n",
    "    source = source.sort_values(by=['dist', 'isInp'], ascending=False)\n",
    "    \n",
    "    logger.debug(f\"source:\\n{source}\")\n",
    "    \n",
    "    logger.debug(f\"dist: {dist}\")\n",
    "    logger.debug(f\"choice:\\n{source.iloc[[dist]]}\")\n",
    "\n",
    "    return source.iloc[[dist]].drop(['dist', 'isInp'], axis=1)\n",
    "\n",
    "#Find result with specific closeness\n",
    "def closeness(inp, source, conf):\n",
    "    if conf == None:\n",
    "        closeness = 1\n",
    "    else:\n",
    "        closeness = conf.get('closeness', 1)\n",
    "        \n",
    "    closeness = min(max(closeness, 0), 2) #0 is equal to the input, 2 is its inverse\n",
    "    \n",
    "    #use pandas to get dists\n",
    "    source[['dist', 'isInp']] = source.apply(lambda row: pd.Series([wv.cosine_similarities(list(inp['vec']), [list(row['vec'])])[0], inp[0] == row[0]]), axis = 1)\n",
    "    resIndex = source['dist'].add(closeness-1).abs().idxmin()\n",
    "    \n",
    "    return source.iloc[[resIndex]].drop(['dist', 'isInp'], axis=1)\n",
    "\n",
    "#Find result closest to inverse input vector\n",
    "def negative(inp, source, conf):\n",
    "    return closeness(inp, source, {'closeness': 2})\n",
    "\n",
    "methods = {\n",
    "    'nodes':{\n",
    "        'random': rand,\n",
    "        'dist': dist,\n",
    "        'closeness': closeness,\n",
    "        'negative': negative\n",
    "    },\n",
    "    'relations': {\n",
    "        'random': rand,\n",
    "        'dist': dist,\n",
    "        'closeness': closeness,\n",
    "        'negative': negative\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertGraph(g):\n",
    "    logger.info(f\"Converting graph\")\n",
    "    length = 0\n",
    "    for s, p, o in g.triples((None, None, None)):\n",
    "        length += 1\n",
    "\n",
    "    fullVectors = []\n",
    "    for s, p, o in tqdm(g.triples((None, None, None)), total=length, file=tqdm_out, mininterval=2):\n",
    "        fullVectors += [[s, p, o]]\n",
    "\n",
    "    return pd.DataFrame(data=fullVectors, columns=['s', 'p', 'o'])\n",
    "\n",
    "def vectorifyGraph(fullVectors):\n",
    "    logger.info(f\"Vecotrifying graph\")\n",
    "\n",
    "    logger.info('Subject vectors')\n",
    "    fullVectors[['s_vec', 's_is_multipart', 's_multipart_%']] = fullVectors.progress_apply(lambda row: pd.Series(toVector(row['s'])), axis=1)\n",
    "\n",
    "    logger.info('Object vectors')\n",
    "    fullVectors[['o_vec', 'o_is_multipart', 'o_multipart_%']] = fullVectors.progress_apply(lambda row: pd.Series(toVector(row['o'])), axis=1)\n",
    "\n",
    "    logger.info('Relation vectors')\n",
    "    fullVectors[['r_vec', 'is_zero_vector_relation']] = fullVectors.progress_apply(lambda row: pd.Series([\n",
    "        row['o_vec']-row['s_vec'] if row['o_vec'] is not None and row['s_vec'] is not None else None,\n",
    "        np.array_equal(row['o_vec']-row['s_vec'], [0]*len(row['o_vec'])) if row['o_vec'] is not None and row['s_vec'] is not None else False\n",
    "    ]), axis=1)\n",
    "    \n",
    "    return fullVectors\n",
    "\n",
    "def calculateNodeEstimates(fullVectors, relationVectors):\n",
    "    logger.info(f\"Calculating node estimates\")\n",
    "    \n",
    "    def helper(row, op, same, other):\n",
    "        #Select the relation vector if there is one\n",
    "        relVecs = relationVectors[relationVectors['relation'] == row['p']]\n",
    "        if len(relVecs) > 0:\n",
    "            rVec = relVecs['vec'].iloc[0]\n",
    "        else:\n",
    "            rVec = None\n",
    "        \n",
    "        #Calculate the estimate\n",
    "        if row[other] is not None and rVec is not None:\n",
    "            est = op(row[other], rVec)\n",
    "        else:\n",
    "            est = None\n",
    "        \n",
    "        #Calculate the distance\n",
    "        if est is not None and row[same] is not None:\n",
    "            dist = wv.cosine_similarities(row[same], [est])[0]\n",
    "        else:\n",
    "            dist = None\n",
    "        \n",
    "        return pd.Series([est, dist], dtype='object')\n",
    "\n",
    "    logger.info(f\"Subject estimates\")\n",
    "    fullVectors[['s_est', 's_est_dist']] = fullVectors.progress_apply(helper, args=[np.subtract, 's_vec', 'o_vec'], axis=1)\n",
    "    \n",
    "    logger.info(f\"Object estimates\")\n",
    "    fullVectors[['o_est', 'o_est_dist']] = fullVectors.progress_apply(helper, args=[np.add, 'o_vec', 's_vec'], axis=1)\n",
    "    \n",
    "    #Workaround for readability as pandas is equaling NaN and None\n",
    "    fullVectors = fullVectors.astype({'s_est': 'object', 's_est_dist': 'object', 'o_est': 'object', 'o_est_dist': 'object'})\n",
    "    fullVectors.loc[fullVectors['s_est'].isna(), 's_est'] = None\n",
    "    fullVectors.loc[fullVectors['s_est_dist'].isna(), 's_est_dist'] = None\n",
    "    fullVectors.loc[fullVectors['o_est'].isna(), 'o_est'] = None\n",
    "    fullVectors.loc[fullVectors['o_est_dist'].isna(), 'o_est_dist'] = None\n",
    "    \n",
    "    return fullVectors\n",
    "\n",
    "def generateNodeVectors(fullVectors):\n",
    "    logger.info(f\"Generating nodeVectors\")\n",
    "    \n",
    "    #Rename and merge\n",
    "    logger.debug(f\"Renaming and merging\")\n",
    "    subjectVectors = fullVectors[['s', 's_vec', 's_is_multipart', 's_multipart_%', 's_est', 's_est_dist']].rename(columns={'s': 'node',\n",
    "                                                                                                    's_vec': 'vec',\n",
    "                                                                                                    's_is_multipart': 'is_multipart',\n",
    "                                                                                                    's_multipart_%': 'multipart_%',\n",
    "                                                                                                    's_est': 'est',\n",
    "                                                                                                    's_est_dist': 'est_dist'})\n",
    "    objectVectors = fullVectors[['o', 'o_vec', 'o_is_multipart', 'o_multipart_%', 'o_est', 'o_est_dist']].rename(columns={'o': 'node',\n",
    "                                                                                                   'o_vec': 'vec',\n",
    "                                                                                                   'o_is_multipart': 'is_multipart',\n",
    "                                                                                                   'o_multipart_%': 'multipart_%',\n",
    "                                                                                                   'o_est': 'est',\n",
    "                                                                                                   'o_est_dist': 'est_dist'})\n",
    "    nodeVectors = pd.concat([subjectVectors, objectVectors], ignore_index=True)\n",
    "    \n",
    "    #Remove duplicates\n",
    "    logger.debug(f\"Grouping\")\n",
    "    nodeGroup = nodeVectors.groupby('node')\n",
    "\n",
    "    logger.debug(f\"Using first for vector\")\n",
    "    #nodeVectors = nodeGroup.first().reset_index() #this is really slow\n",
    "    nodeVectors = nodeGroup.head(1).reset_index(drop=True)\n",
    "    \n",
    "    logger.debug(f\"Calculating totals\")\n",
    "    nodeVectors['total'] = nodeGroup.size().reset_index(drop=True)\n",
    "    \n",
    "    logger.debug(f\"Calculating estimates\")\n",
    "    nodeVectors['est'] = nodeGroup['est'].apply(np.mean).reset_index(drop=True)\n",
    "    \n",
    "    #Workaround as pandas is equaling NaN and None\n",
    "    nodeVectors = nodeVectors.astype({'est': 'object', 'vec': 'object'})\n",
    "    nodeVectors.loc[nodeVectors['est'].isna(), 'est'] = None\n",
    "    nodeVectors.loc[nodeVectors['vec'].isna(), 'vec'] = None\n",
    "    \n",
    "    logger.debug(f\"Calculating estimate distances\")\n",
    "    nodeVectors['est_dist'] = nodeVectors.apply(lambda row: wv.cosine_similarities(list(row['est']), [list(row['vec'])])[0] if row['est'] is not None and row['vec'] is not None else None, axis=1)\n",
    "    \n",
    "    logger.debug(f\"Calculating mean/min/max of distances\")\n",
    "    nodeVectors['mean_est_dist'] = nodeGroup['est_dist'].apply(np.mean).reset_index(drop=True)\n",
    "    nodeVectors['min_est_dist'] = nodeGroup['est_dist'].apply(np.min).reset_index(drop=True)\n",
    "    nodeVectors['max_est_dist'] = nodeGroup['est_dist'].apply(np.max).reset_index(drop=True)\n",
    "    \n",
    "    #Split into nodeVectors and lostNodes\n",
    "    logger.debug(f\"Splitting into nodeVectors and lostNodes\")\n",
    "    lostNodes = nodeVectors[nodeVectors['vec'].isnull()].reset_index(drop=True).drop(columns=['vec', 'est_dist', 'mean_est_dist', 'min_est_dist', 'max_est_dist'])\n",
    "    nodeVectors = nodeVectors.dropna().reset_index(drop=True)\n",
    "    logger.info(\"Done\")\n",
    "    \n",
    "    return nodeVectors, lostNodes\n",
    "\n",
    "def generateRelationVectors(fullVectors):\n",
    "    logger.info(f\"Generating relationVectors\")\n",
    "    \n",
    "    logger.debug(f\"Grouping\")\n",
    "    relationVectors = fullVectors.groupby('p')['r_vec'].apply(np.mean).reset_index().rename(columns={'p': 'relation', 'r_vec': 'vec'})\n",
    "\n",
    "    logger.debug(f\"Calculating total, lost, zero_vector and quality\")\n",
    "    relationVectors['total'] = fullVectors.groupby('p')['p'].count().reset_index(drop=True)\n",
    "    relationVectors['lost'] = fullVectors.groupby('p')['r_vec'].apply(lambda x: x.isnull().sum()).reset_index(drop=True)\n",
    "    relationVectors['zero_vector'] = fullVectors.groupby('p')['is_zero_vector_relation'].sum().astype(int).reset_index(drop=True)\n",
    "    relationVectors['quality'] = relationVectors.apply(lambda row: 1-(row['lost']+row['zero_vector'])/row['total'], axis=1)\n",
    "\n",
    "    #TODO: labels are counted as lost nodes\n",
    "    #Split into relationVectors and lostRelations\n",
    "    logger.debug(f\"Splitting into relationVectors and lostRelations\")\n",
    "    lostRelations = relationVectors[np.bitwise_or(\n",
    "        relationVectors['vec'].isnull(),\n",
    "        relationVectors['total']-relationVectors['lost']-relationVectors['zero_vector'] == 0\n",
    "    )].reset_index(drop=True).drop(columns=['vec', 'quality'])\n",
    "    relationVectors = relationVectors[np.bitwise_and(\n",
    "        relationVectors['vec'].notnull(),\n",
    "        relationVectors['total']-relationVectors['lost']-relationVectors['zero_vector'] != 0\n",
    "    )].reset_index(drop=True)\n",
    "    \n",
    "    #Min/max/average distance of every full vector of this relation type to mean vector\n",
    "    logger.debug(f\"Calculating min, max and average distances\")\n",
    "    def helper_dist(row):\n",
    "        vectors = fullVectors[fullVectors['p'] == row['relation']]\n",
    "\n",
    "        #filter out None and zero-vector\n",
    "        vectors = vectors[vectors['is_zero_vector_relation'] == False]\n",
    "        vectors = vectors['r_vec'].dropna().reset_index(drop=True)\n",
    "\n",
    "        sims = wv.cosine_similarities(row['vec'], list(vectors))\n",
    "        return [np.min(sims), np.max(sims), np.mean(sims)]\n",
    "\n",
    "    relationVectors[['min_dist', 'max_dist', 'mean_dist']] = relationVectors.apply(lambda row: pd.Series(helper_dist(row)), axis=1)\n",
    "    logger.info(\"Done\")\n",
    "    \n",
    "    return relationVectors, lostRelations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertedGraph = convertGraph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fullVectors = vectorifyGraph(convertedGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationVectors, lostRelations = generateRelationVectors(fullVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullVectors = calculateNodeEstimates(fullVectors, relationVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeVectors, lostNodes = generateNodeVectors(fullVectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & load dataframes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "fullVectors.to_pickle(\"fullVectors.plk\")\n",
    "nodeVectors.to_pickle(\"nodeVectors.plk\")\n",
    "lostNodes.to_pickle(\"lostNodes.plk\")\n",
    "relationVectors.to_pickle(\"relationVectors.plk\")\n",
    "lostRelations.to_pickle(\"lostRelations.plk\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "fullVectors = pd.read_pickle(\"fullVectors.plk\")\n",
    "nodeVectors = pd.read_pickle(\"nodeVectors.plk\")\n",
    "lostNodes = pd.read_pickle(\"lostNodes.plk\")\n",
    "relationVectors = pd.read_pickle(\"relationVectors.plk\")\n",
    "lostRelations = pd.read_pickle(\"lostRelations.plk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View calculated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length: {len(fullVectors)}\")\n",
    "fullVectors.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show rows where the relation vector was lost\n",
    "fullVectors[fullVectors['r_vec'].isnull()].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show rows where the relation vector is a zero-vector\n",
    "fullVectors[fullVectors['is_zero_vector_relation'] == True].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length: {len(nodeVectors)}\")\n",
    "nodeVectors.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "filt = np.bitwise_and(nodeVectors['est_dist'] < 0.5, nodeVectors['est_dist'] > -0.5)\n",
    "filt = np.bitwise_and(filt, nodeVectors['total'] > 5)\n",
    "\n",
    "print(f\"Length: {len(nodeVectors[filt])}\")\n",
    "nodeVectors[filt].sort_values(by=['total'], ascending=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.random.seed(0)\n",
    "rndNodes = nodeVectors.sample(n=2, replace=True)\n",
    "nodeSel = pd.DataFrame([\n",
    "    nodeVectors.iloc[2438],\n",
    "    nodeVectors.iloc[1489],\n",
    "    nodeVectors.iloc[4331],\n",
    "    #nodeVectors.iloc[4218],\n",
    "    #nodeVectors.iloc[3441],\n",
    "])\n",
    "nodeSel = pd.concat([nodeSel, rndNodes])\n",
    "\n",
    "nodeSel.sort_index().head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "i = 18493\n",
    "fullVectors[np.bitwise_or(fullVectors['o'] == nodeVectors.iloc[i]['node'], fullVectors['s'] == nodeVectors.iloc[i]['node'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Length: {len(lostNodes)}\")\n",
    "lostNodes.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Relation Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length: {len(relationVectors)}\")\n",
    "relationVectors.head(37)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "filt = np.bitwise_and(relationVectors['mean_dist'] < 0.5, relationVectors['total'] > 5)\n",
    "filt = np.bitwise_and(filt, relationVectors['mean_dist'] > -0.5)\n",
    "\n",
    "print(f\"Length: {len(relationVectors[filt])}\")\n",
    "relationVectors[filt].sort_values(by=['total'], ascending=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.random.seed(0)\n",
    "rndRels = relationVectors.sample(n=2, replace=True)\n",
    "relSel = pd.DataFrame([\n",
    "    relationVectors.iloc[9],\n",
    "    relationVectors.iloc[23],\n",
    "    relationVectors.iloc[20],\n",
    "    #relationVectors.iloc[10],\n",
    "    #relationVectors.iloc[19],\n",
    "])\n",
    "relSel = pd.concat([relSel, rndRels])\n",
    "\n",
    "relSel.sort_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Length: {len(lostRelations)}\")\n",
    "lostRelations.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual calculations for verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Full vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multipart\n",
    "print(nodeVectors['is_multipart'].iloc[20])\n",
    "print(nodeVectors['multipart_%'].iloc[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zero-Vector\n",
    "print(fullVectors['is_zero_vector_relation'].iloc[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lost relation\n",
    "print(fullVectors['r_vec'].iloc[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Estimate\n",
    "print(fullVectors['s_est'].iloc[18], fullVectors['s_est_dist'].iloc[18])\n",
    "\n",
    "#print(fullVectors['s_est'].iloc[0])\n",
    "\n",
    "#Calculate estimate (minus means arrow from R to L for relationVector calculation)\n",
    "est = fullVectors['o_vec'].iloc[0]-relationVectors['vec'].iloc[2]\n",
    "#print(est)\n",
    "\n",
    "print(wv.cosine_similarities(fullVectors['s_est'].iloc[0], [est])[0])\n",
    "\n",
    "print(fullVectors['s_est_dist'].iloc[0])\n",
    "print(wv.cosine_similarities(fullVectors['s_vec'].iloc[0], [est])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lost relation\n",
    "print(lostRelations['lost'].iloc[0])\n",
    "print(lostRelations['zero_vector'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quality\n",
    "print(relationVectors['quality'].iloc[1])\n",
    "print((relationVectors['total'].iloc[1]-relationVectors['lost'].iloc[1]-relationVectors['zero_vector'].iloc[1])/relationVectors['total'].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Unlabeled\n",
    "np.array_equal(fullVectors['o_vec'].iloc[32], wv['australia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lost node\n",
    "lostNodes.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Node with the maximal distance to 'berlin'\n",
    "nodeVectors.iloc[np.argmin(wv.cosine_similarities(wv['berlin'], list(nodeVectors['vec'])))]['node']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Dist method\n",
    "\n",
    "#Same\n",
    "print(nodeVectors['node'].iloc[0], dist(nodeVectors.iloc[0].copy(), nodeVectors.copy(), {'dist': 0})['node'].iloc[0])\n",
    "\n",
    "#Inverse\n",
    "print(nodeVectors['node'].iloc[0], dist(nodeVectors.iloc[0].copy(), nodeVectors.copy(), {'dist': 100})['node'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closeness method\n",
    "\n",
    "#Same\n",
    "print(nodeVectors['node'].iloc[0], closeness(nodeVectors.iloc[0].copy(), nodeVectors.copy(), {'closeness': 0})['node'].iloc[0])\n",
    "\n",
    "#Inverse\n",
    "print(nodeVectors['node'].iloc[0], closeness(nodeVectors.iloc[0].copy(), nodeVectors.copy(), {'closeness': 2})['node'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ambiguity\n",
    "print(\"0.1191733359168643\")\n",
    "\n",
    "#for relations\n",
    "weights = relationVectors['total'] - relationVectors['lost'] - relationVectors['zero_vector']\n",
    "relAmbig = np.average(relationVectors['mean_dist'], weights=weights)\n",
    "\n",
    "print(np.array_equal(weights, [6, 5, 7]))\n",
    "print(relAmbig)\n",
    "print((relationVectors['mean_dist'].iloc[0]*6+relationVectors['mean_dist'].iloc[1]*5+relationVectors['mean_dist'].iloc[2]*7)/(6+5+7))\n",
    "\n",
    "#for nodes\n",
    "weights = nodeVectors['total']\n",
    "nodeAmbig = np.average(nodeVectors['est_dist'], weights=weights)\n",
    "\n",
    "#mean of ambiguities of nodes and vectors, weight by nodes 2:1 relation\n",
    "totAmbig = np.average([relAmbig, nodeAmbig], weights=[2, 1])\n",
    "\n",
    "#transform -1..1 where 1 is the least ambiguous to 0..1 where 1 is the most ambiguous\n",
    "print(1-((1+totAmbig)/2))\n",
    "\n",
    "print(1-((1+(-1))/2))\n",
    "print(1-((1+(1))/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate small natural ambiguity\n",
    "config = {\n",
    "    'nodes': {\n",
    "        'random': [{'amount': {'num': 2}}],\n",
    "        'dist': [{\n",
    "                    'amount': {'num': 10},\n",
    "                     'param': {'dist': 1}\n",
    "                 }, {\n",
    "                     'amount': {'num': 2},\n",
    "                     'param': {'dist': 2}\n",
    "                 }]\n",
    "    },\n",
    "    'relations': {\n",
    "        'random': [{'amount': {'num': 2}}],\n",
    "        'dist': [{'amount': {'num': 5},\n",
    "                 'param': {'dist': 1}}]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate medium natural ambiguity\n",
    "config = {\n",
    "    'nodes': {\n",
    "        'random': [{'amount': {'num': 2}}],\n",
    "        'dist': [{\n",
    "                    'amount': {'num': 10},\n",
    "                     'param': {'dist': 1}\n",
    "                 }, {\n",
    "                     'amount': {'num': 2},\n",
    "                     'param': {'dist': 2}\n",
    "                 }]\n",
    "    },\n",
    "    'relations': {\n",
    "        'random': [{'amount': {'num': 2}}],\n",
    "        'dist': [{'amount': {'num': 5},\n",
    "                 'param': {'dist': 1}}]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate large natural ambiguity\n",
    "config = {\n",
    "    'nodes': {\n",
    "        'random': [{'amount': {'num': 2}}],\n",
    "        'dist': [{\n",
    "                    'amount': {'num': 10},\n",
    "                     'param': {'dist': 1}\n",
    "                 }, {\n",
    "                     'amount': {'num': 2},\n",
    "                     'param': {'dist': 2}\n",
    "                 }]\n",
    "    },\n",
    "    'relations': {\n",
    "        'random': [{'amount': {'num': 2}}],\n",
    "        'dist': [{'amount': {'num': 5},\n",
    "                 'param': {'dist': 1}}]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Random changes for testing\n",
    "config = {\n",
    "    'nodes': {\n",
    "        'random': [{'amount': {'num': 5000}}]\n",
    "    },\n",
    "    'relations': {\n",
    "        'random': [{'amount': {'num': 2500}}]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Random changes for testing\n",
    "config = {\n",
    "    'nodes': {\n",
    "        'random': [{'amount': {'num': 5}}]\n",
    "    },\n",
    "    'relations': {\n",
    "        'random': [{'amount': {'num': 2}}]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Complete negative for testing\n",
    "config = {\n",
    "    'nodes': {\n",
    "        'negative': [{'amount': {'perc': 1}}]\n",
    "    },\n",
    "    'relations': {\n",
    "        'negative': [{'amount': {'perc': 1}}]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Closeness for testing\n",
    "config = {\n",
    "    'nodes': {\n",
    "        'closeness': [{'amount': {'num': 5},\n",
    "                 'param': {'closeness': 0.2}}]\n",
    "    },\n",
    "    'relations': {\n",
    "        'closeness': [{'amount': {'num': 5},\n",
    "                 'param': {'closeness': 0.2}}]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate different amounts of ambiguity\n",
    "config = {\n",
    "    'nodes': {\n",
    "        'random': [{'amount': {'num': 5}}],\n",
    "        'dist': [{\n",
    "                    'amount': {'num': 5},\n",
    "                     'param': {'dist': 1}\n",
    "                 }, {\n",
    "                     'amount': {'num': 5},\n",
    "                     'param': {'dist': 2}\n",
    "                 }, {\n",
    "                     'amount': {'num': 5},\n",
    "                     'param': {'dist': 3}\n",
    "                 }, {\n",
    "                     'amount': {'num': 5},\n",
    "                     'param': {'dist': 4}\n",
    "                 }, {\n",
    "                     'amount': {'num': 5},\n",
    "                     'param': {'dist': 5}\n",
    "                 }]\n",
    "    },\n",
    "    'relations': {\n",
    "        'random': [{'amount': {'num': 5}}],\n",
    "        'dist': [{\n",
    "                    'amount': {'num': 5},\n",
    "                    'param': {'dist': 1}\n",
    "                 },{\n",
    "                    'amount': {'num': 5},\n",
    "                    'param': {'dist': 2}\n",
    "                },{\n",
    "                    'amount': {'num': 5},\n",
    "                    'param': {'dist': 3}\n",
    "                },{\n",
    "                    'amount': {'num': 5},\n",
    "                    'param': {'dist': 4}\n",
    "                },{\n",
    "                    'amount': {'num': 5},\n",
    "                    'param': {'dist': 5}\n",
    "                }]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate different amounts of ambiguity\n",
    "config = {\n",
    "    'nodes': {\n",
    "        'random': [{'amount': {'num': 5}}],\n",
    "        'dist': [{\n",
    "                    'amount': {'num': 5},\n",
    "                     'param': {'dist': 0}\n",
    "                 }, {\n",
    "                     'amount': {'num': 5},\n",
    "                     'param': {'dist': 1}\n",
    "                 }, {\n",
    "                     'amount': {'num': 5},\n",
    "                     'param': {'dist': 2}\n",
    "                 }, {\n",
    "                     'amount': {'num': 3},\n",
    "                     'param': {'dist': 3}\n",
    "                 }, {\n",
    "                     'amount': {'num': 3},\n",
    "                     'param': {'dist': 4}\n",
    "                 }, {\n",
    "                     'amount': {'num': 2},\n",
    "                     'param': {'dist': 10}\n",
    "                 }, {\n",
    "                     'amount': {'num': 2},\n",
    "                     'param': {'dist': 50}\n",
    "                 }]\n",
    "    },\n",
    "    'relations': {\n",
    "        'random': [{'amount': {'num': 5}}],\n",
    "        'dist': [{\n",
    "                    'amount': {'num': 5},\n",
    "                     'param': {'dist': 0}\n",
    "                 }, {\n",
    "                     'amount': {'num': 5},\n",
    "                     'param': {'dist': 1}\n",
    "                 }, {\n",
    "                     'amount': {'num': 5},\n",
    "                     'param': {'dist': 2}\n",
    "                 }, {\n",
    "                     'amount': {'num': 3},\n",
    "                     'param': {'dist': 3}\n",
    "                 }, {\n",
    "                     'amount': {'num': 3},\n",
    "                     'param': {'dist': 4}\n",
    "                 }, {\n",
    "                     'amount': {'num': 2},\n",
    "                     'param': {'dist': 10}\n",
    "                 }, {\n",
    "                     'amount': {'num': 2},\n",
    "                     'param': {'dist': 50}\n",
    "                 }]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Run imports and define functions, configure the desired result\n",
    "\n",
    "#Load graph and dictionary, you can skip this if you already ran it\n",
    "convertedGraph = convertGraph(g)\n",
    "fullVectors = vectorifyGraph(convertedGraph)\n",
    "relationVectors, lostRelations = generateRelationVectors(fullVectors)\n",
    "fullVectors = calculateNodeEstimates(fullVectors, relationVectors)\n",
    "nodeVectors, lostNodes = generateNodeVectors(fullVectors)\n",
    "\n",
    "#Check outputs before continuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#The ambiguify-function returns vectors according to configured methods\n",
    "np.random.seed(0)\n",
    "changes = ambiguify(config, nodeVectors, relationVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert new node into graph based on one random triple containing the source\n",
    "g2 = Graph()\n",
    "np.random.seed(0)\n",
    "additions = populateAdditions(changes, g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Save additions from nodes and relations\n",
    "logger.info(f\"Saving files\")\n",
    "\n",
    "f = open(\"additions.ttl\", \"wb\")\n",
    "f.write(g2.serialize(format='turtle'))\n",
    "f.close()\n",
    "\n",
    "#Save graph with additions\n",
    "g3 = g+g2\n",
    "f = open(\"appendedKG.ttl\", \"wb\")\n",
    "f.write(g3.serialize(format='turtle'))\n",
    "f.close()\n",
    "\n",
    "logger.info(f\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate ambiguity before\n",
    "ambiguityBefore = calculateAmbiguity(fullVectors, nodeVectors, relationVectors)\n",
    "logger.info(f\"Ambiguity before: {ambiguityBefore}\")\n",
    "\n",
    "#Prepare data to calculate ambiguity after\n",
    "newFullVectors = fullVectors.copy()\n",
    "\n",
    "if len(additions) > 0:\n",
    "    logger.info(f\"Adding {len(additions)} additional triples\")\n",
    "    vectorisedAdditions = vectorifyGraph(additions)\n",
    "    newFullVectors = newFullVectors.append(vectorisedAdditions, ignore_index = True)\n",
    "\n",
    "newRelationVectors, newLostRelations = generateRelationVectors(newFullVectors)\n",
    "newFullVectors = calculateNodeEstimates(newFullVectors, newRelationVectors)\n",
    "newNodeVectors, newLostNodes = generateNodeVectors(newFullVectors)\n",
    "\n",
    "#Calculate ambiguity after\n",
    "ambiguityAfter = calculateAmbiguity(newFullVectors, newNodeVectors, newRelationVectors)\n",
    "logger.info(f\"Ambiguity after: {ambiguityAfter}\")\n",
    "\n",
    "logger.info(f\"Ambiguity difference: {ambiguityAfter-ambiguityBefore}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export for AMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate different severities for same original triple\n",
    "\n",
    "def ambiguify4amt(config, nodeVectors, relationVectors):\n",
    "    out = pd.DataFrame()\n",
    "    for target in config: #can be \"nodes\" or \"relations\"\n",
    "        if target == 'nodes':\n",
    "            inp = nodeVectors\n",
    "            selres = nodeSel\n",
    "        else:\n",
    "            inp = relationVectors\n",
    "            selres = relSel\n",
    "            \n",
    "        selres = inp.sample(n=5, replace=True) # hardcoded for evaluation\n",
    "        #print(selres.head(100))\n",
    "        \n",
    "        for method in config[target]: #matches the name of the method\n",
    "            for instance in config[target][method]: #once for every instance of the method config\n",
    "\n",
    "                logger.debug(f\"Selres: {selres}\")\n",
    "                \n",
    "                for val in instance['amount']: #the amount of elements to be changed\n",
    "                    logger.info(f\"Ambiguifying {target} with {method} (parameters: {instance})\")\n",
    "                    \n",
    "                    conf = instance.get('param', None)\n",
    "                    sourceColumn = target[0:-1]\n",
    "                    rep = pd.DataFrame()\n",
    "                    \n",
    "                    rep[['method', 'config', 'source_type', 'source', 'target']] = selres.progress_apply(lambda sel: pd.Series([\n",
    "                        method,\n",
    "                        str(instance),\n",
    "                        sourceColumn,\n",
    "                        sel[sourceColumn],\n",
    "                        methods[target][method](sel.copy(), inp.copy(), conf)[sourceColumn].iloc[0]\n",
    "                    ]), axis=1)\n",
    "                    \n",
    "                    out = out.append(rep, ignore_index=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "changes = ambiguify4amt(config, nodeVectors, relationVectors)\n",
    "\n",
    "g2 = Graph()\n",
    "np.random.seed(0)\n",
    "additions = populateAdditions(changes, g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change format\n",
    "amt = pd.concat([changes, additions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute names\n",
    "def getPreferredTitle4amt(n, lang=\"en\"):\n",
    "    label = g.preferredLabel(n, lang=lang)\n",
    "\n",
    "    #if type(n) is not type(Literal(\"\")): #only labels should be of type literal\n",
    "    if label == []:\n",
    "        return n.rsplit('/', 1)[-1].replace('_', ' ').replace(',', '')\n",
    "    else:\n",
    "        return label[0][1].value\n",
    "\n",
    "\n",
    "amt['s_orig_name'] = amt['s_orig'].apply(getPreferredTitle4amt)\n",
    "amt['p_orig_name'] = amt['p_orig'].apply(getPreferredTitle4amt)\n",
    "amt['o_orig_name'] = amt['o_orig'].apply(getPreferredTitle4amt)\n",
    "\n",
    "amt['s_name'] = amt['s'].apply(getPreferredTitle4amt)\n",
    "amt['p_name'] = amt['p'].apply(getPreferredTitle4amt)\n",
    "amt['o_name'] = amt['o'].apply(getPreferredTitle4amt)\n",
    "\n",
    "#Compute verification code\n",
    "def ver4amt(row):\n",
    "    num = np.array([\n",
    "        np.random.randint(np.min([len(row['s_name']), 5])-1),\n",
    "        np.random.randint(np.min([len(row['p_name']), 5])-1),\n",
    "        np.random.randint(np.min([len(row['o_name']), 5])-1)\n",
    "    ])\n",
    "    \n",
    "    code = (row['s_name'][num[0]]+row['p_name'][num[1]]+row['o_name'][num[2]]).replace(' ', '_')\n",
    "\n",
    "    #print(row[['s_name', 'p_name', 'o_name']], num+1, code)\n",
    "    return ('-'.join(str(x) for x in (num+1)), code)\n",
    "\n",
    "np.random.seed(0)\n",
    "amt[['ver_num', 'ver_code']] = amt.apply(lambda row: pd.Series(ver4amt(row)), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amt.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes.to_pickle(\"changes.plk\")\n",
    "additions.to_pickle(\"additions.plk\")\n",
    "amt.to_pickle(\"amt.plk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amt.to_csv(\"amt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process AMT results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amtres = pd.read_csv(\"Batch_305165_batch_results.csv\") #pilot 1\n",
    "amtres = pd.read_csv(\"Batch_305480_batch_results.csv\") #pilot 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set global plot size\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistSeverity(sourceType, source, target):\n",
    "    if sourceType == 'node':\n",
    "        inp = nodeVectors.copy()\n",
    "    else:\n",
    "        inp = relationVectors.copy()\n",
    "    \n",
    "    sourceEntry = inp[inp[sourceType] == URIRef(source)]\n",
    "    \n",
    "    #print(f\"SourceEntry: {sourceEntry.head()}\")\n",
    "    \n",
    "    inp['dist'] = inp.apply(lambda row: #print(f\"{list(sourceEntry['vec'])[0]}, {[list(row['vec'])]}\"),\n",
    "                              wv.cosine_similarities(list(sourceEntry['vec'])[0], [list(row['vec'])])[0]\n",
    "                              , axis = 1)\n",
    "    inp = inp.sort_values(by=['dist'], ascending=False).reset_index(drop=True)\n",
    "    #print(inp.head(5))\n",
    "    \n",
    "    targetEntry = inp[inp[sourceType] == URIRef(target)]\n",
    "    #print(f\"{targetEntry.head()}\\nIndex: {targetEntry.index[0]}\")\n",
    "        \n",
    "    return targetEntry.index[0] #TODO: calculate actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeVectors.iloc[3031]['node']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(nodeVectors.iloc[3031]['node'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeVectors[nodeVectors['node'] == URIRef('http://dbpedia.org/resource/Robert_Cary_(director)')]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def dist(inp, source, conf):\n",
    "    if conf == None:\n",
    "        dist = 1\n",
    "    else:\n",
    "        dist = conf.get('dist', 1)\n",
    "        \n",
    "    dist = min(max(dist, 0), len(source.index)-1)\n",
    "    \n",
    "    #use pandas to get top-n, if dist is same move inp to the top\n",
    "    source[['dist', 'isInp']] = source.apply(lambda row: pd.Series([wv.cosine_similarities(list(inp['vec']), [list(row['vec'])])[0], inp[0] == row[0]]), axis = 1)\n",
    "    source = source.sort_values(by=['dist', 'isInp'], ascending=False)\n",
    "    \n",
    "    logger.debug(f\"source:\\n{source}\")\n",
    "    \n",
    "    logger.debug(f\"dist: {dist}\")\n",
    "    logger.debug(f\"choice:\\n{source.iloc[[dist]]}\")\n",
    "\n",
    "    return source.iloc[[dist]].drop(['dist', 'isInp'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract and format data for plots\n",
    "\n",
    "amtres['severity'] = amtres.apply(lambda row: getDistSeverity(row['Input.source_type'], row['Input.source'], row['Input.target'])\n",
    "                                  if row['Input.method'] == 'random'\n",
    "                                  else (json.loads(row['Input.config'].replace(\"'\", '\"'))['param']['dist']\n",
    "                                        if row['Input.method'] == 'dist'\n",
    "                                        else -1)\n",
    "                                  , axis = 1)\n",
    "\n",
    "amtres['Answer.mistakeSource'] = amtres.apply(lambda row: 'artificial'\n",
    "                                  if row['Answer.mistakeSource.artificial'] == True\n",
    "                                  else ( 'human'\n",
    "                                        if row['Answer.mistakeSource.human'] == True\n",
    "                                        else 'correct')\n",
    "                                  , axis = 1)\n",
    "\n",
    "data = amtres[['Answer.stars', 'WorkTimeInSeconds', 'severity']]#.sort_values(['Answer.stars', 'WorkTimeInSeconds'])\n",
    "\n",
    "#Input.method\n",
    "#Input.source_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "amtres.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total answers per rating\n",
    "amtres[['Answer.stars', 'WorkerId']].groupby('Answer.stars').count().reset_index().rename(columns={'Answer.stars': 'Rating', 'WorkerId': 'count'}).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total answers per rating\n",
    "plt.plot(amtres[['Answer.stars', 'WorkerId']].groupby('Answer.stars').count())\n",
    "plt.title('Total answers per rating');\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Number of answers')\n",
    "\n",
    "#TODO: one line per worker and avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total answers per rating\n",
    "amtres[['Answer.mistakeSource', 'WorkerId', 'Input.method']].groupby(['Input.method', 'Answer.mistakeSource']).count().reset_index().rename(columns={'WorkerId': 'count', 'Answer.mistakeSource': 'mistakeSource'}).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answers with wrong code\n",
    "amtres[amtres['Answer.code'] != amtres['Input.ver_code']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: one table with wrong/correct per worker\n",
    "amtCodeCheck = pd.DataFrame()\n",
    "\n",
    "amtCodeCheck[['WorkerId', 'correct']] = amtres[amtres['Answer.code'] != amtres['Input.ver_code']][['WorkerId', 'Answer.stars']].groupby('WorkerId').count().reset_index()\n",
    "amtCodeCheck[['WorkerId', 'incorrect']] = amtres[amtres['Answer.code'] == amtres['Input.ver_code']][['WorkerId', 'Answer.stars']].groupby('WorkerId').count().reset_index()\n",
    "\n",
    "amtCodeCheck.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean worktime per rating of all wrong codes\n",
    "amtres[amtres['Answer.code'] != amtres['Input.ver_code']][['Answer.stars', 'WorkTimeInSeconds']].groupby('Answer.stars').mean().reset_index().head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#Mean worktime per rating of intentionally wrong codes\n",
    "amtres[amtres['Answer.code'] == 'xxx'][['Answer.stars', 'WorkTimeInSeconds']].groupby('Answer.stars').mean().reset_index().head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#Answers per rating of specific worker\n",
    "amtres[amtres['WorkerId'] == 'A1R3XGPH6EK6G6'][['Answer.stars', 'severity']].groupby('Answer.stars').count().reset_index().head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Correlation of a specific worker\n",
    "amtres[amtres['WorkerId'] == 'A1R3XGPH6EK6G6'][['Answer.stars', 'severity', 'WorkTimeInSeconds']].corr()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Save graphs\n",
    "fig = data.plot().get_figure()\n",
    "fig.savefig('figures/timeVsStars.png')\n",
    "\n",
    "plt.savefig('figures/timeVsStars.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.scatter(data['Answer.stars'], data['severity'], label='a')\n",
    "plt.title(\"a\");\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Severity\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.scatter(data['WorkTimeInSeconds'], data['severity'], label='a')\n",
    "plt.title(\"a\");\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Worktime\")\n",
    "plt.ylabel(\"Severity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation excluding random\n",
    "#amtres.groupby('Input.method')[['Answer.stars', 'WorkTimeInSeconds', 'severity']].corr()\n",
    "\n",
    "amtres[amtres['Input.method'] == 'dist'][['Answer.stars', 'WorkTimeInSeconds', 'severity']].corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation for random\n",
    "amtres[amtres['Input.method'] == 'random'][['Answer.stars', 'WorkTimeInSeconds', 'severity']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation including random\n",
    "amtres[['Answer.stars', 'WorkTimeInSeconds', 'severity']].corr()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Correlation test\n",
    "amtres.groupby('Answer.stars')[['Answer.stars', 'WorkTimeInSeconds', 'severity']].corr()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Correlation graphic\n",
    "plt.matshow(data.corr())\n",
    "plt.colorbar() #cb = \n",
    "#cb.ax.tick_params(labelsize=14)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.boxplot(data)\n",
    "plt.title(\"a\");\n",
    "#plt.legend(['a', 'b','c'],  loc=\"upper left\")\n",
    "#plt.xlabel(\"Stars\")\n",
    "#plt.ylabel(\"Severity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "amtres[['Answer.stars', 'severity']].groupby('Answer.stars').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Severities per rating\n",
    "amtres[['severity', 'Answer.stars']].groupby('Answer.stars').boxplot(subplots=False, rot=90, fontsize=12)\n",
    "#amtres[['severity', 'Answer.stars']].boxplot()\n",
    "plt.title(\"Severities per rating\");\n",
    "#plt.legend(['a', 'b','c'],  loc=\"upper left\")\n",
    "plt.xlabel(\"Stars\")\n",
    "plt.ylabel(\"Severity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Rating per severity\n",
    "\n",
    "amtres[['Answer.stars', 'severity']].groupby('severity').boxplot(subplots=False, rot=90, fontsize=12)\n",
    "plt.title(\"Rating per severity\");\n",
    "#plt.legend(['a', 'b','c'],  loc=\"upper left\")\n",
    "plt.xlabel(\"Severity\")\n",
    "plt.ylabel(\"Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Worktime per severity of intentionally wrong codes\n",
    "amtres[['WorkTimeInSeconds', 'severity']].groupby('severity').boxplot(subplots=False, rot=90, fontsize=12)\n",
    "plt.title(\"Worktime per severity\");\n",
    "#plt.legend(['a', 'b','c'],  loc=\"upper left\")\n",
    "plt.xlabel(\"Severity\")\n",
    "plt.ylabel(\"Worktime [s]\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.scatter(data['Answer.stars'], data['severity'], label='a')\n",
    "plt.title(\"a\");\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Stars\")\n",
    "plt.ylabel(\"Severity\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n, bins, patches = amtres[amtres['Answer.code'] == 'xxx'][['WorkTimeInSeconds', 'severity']].groupby('severity').hist(subplots=False, rot=45, fontsize=12)\n",
    "plt.title(\"Histogram\");\n",
    "#plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Amount\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n, bins, patches = plt.hist(data, label=['Answer.stars', 'WorkTimeInSeconds', 'severity'])\n",
    "plt.title(\"Histogram\");\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set accept/reject for amt answers and export\n",
    "\n",
    "#Reject everything with wrong codes\n",
    "\n",
    "\n",
    "#Reject obviously wrong data\n",
    "#TODO: how will we define this? Just ignore it? Recognise correct statements\n",
    "\n",
    "#Export file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check dictionary quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare all dictionaries\n",
    "dicts = {\"gn\": gn, \"gw1\": gw1, \"gw3\": gw3}\n",
    "\n",
    "for i in dicts:\n",
    "    dic = dicts[i]\n",
    "    \n",
    "    #Minus means arrow from R to L\n",
    "    hasCapital = dic[\"tokyo\"] - dic[\"japan\"]\n",
    "    isCapitalOf = dic[\"japan\"] - dic[\"tokyo\"]\n",
    "    \n",
    "    #Calculate results of relation\n",
    "    est_france = dic[\"paris\"] + isCapitalOf\n",
    "    est_paris = dic[\"france\"] + hasCapital\n",
    "    \n",
    "    print(f\"Dictionary: {i}\")\n",
    "    print(f\"est_France to France: {dic.cosine_similarities(est_france, [dic['france']])}\")\n",
    "    print(f\"est_Paris to Paris: {dic.cosine_similarities(est_paris, [dic['paris']])}\")\n",
    "    print(f\"est_France to Paris: {dic.cosine_similarities(est_france, [dic['paris']])}\")\n",
    "    print(f\"est_Paris to France: {dic.cosine_similarities(est_paris, [dic['france']])}\")\n",
    "    print(f\"Paris to France: {dic.cosine_similarities(dic['paris'], [dic['france']])}\")\n",
    "    print(f\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Similarity: {wv.cosine_similarities(wv['white'], [wv['black']])[0]}\\n\")\n",
    "\n",
    "print(\"Inverted node 'white':\")\n",
    "for d in wv.similar_by_vector(vector = -1*wv['white']):\n",
    "    print(f\"Distance of '{d[0]}' to !white: {d[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Currently unused"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Build subset of dictionary, might improve performance\n",
    "def restrict_w2v(w2v, restricted_word_set):\n",
    "    new_vectors = []\n",
    "    new_vocab = {}\n",
    "    new_index2entity = []\n",
    "    new_vectors_norm = []\n",
    "\n",
    "    for i in range(len(w2v.vocab)):\n",
    "        word = w2v.index2entity[i]\n",
    "        vec = w2v.vectors[i]\n",
    "        vocab = w2v.vocab[word]\n",
    "        vec_norm = w2v.vectors_norm[i]\n",
    "        if word in restricted_word_set:\n",
    "            vocab.index = len(new_index2entity)\n",
    "            new_index2entity.append(word)\n",
    "            new_vocab[word] = vocab\n",
    "            new_vectors.append(vec)\n",
    "            new_vectors_norm.append(vec_norm)\n",
    "\n",
    "    w2v.vocab = new_vocab\n",
    "    w2v.vectors = np.array(new_vectors)\n",
    "    w2v.index2entity = np.array(new_index2entity)\n",
    "    w2v.index2word = np.array(new_index2entity)\n",
    "    w2v.vectors_norm = np.array(new_vectors_norm)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Use dictionary to find similar vectors\n",
    "def most_similar_to_given_vec(vec, entities_list):\n",
    "    return wv.similar_by_vector(vector = entities_list[np.argmax([wv.cosine_similarities(vec, [entity])[0] for entity in entities_list])], topn=1)[0][0]\n",
    "\n",
    "# Demo for most_similar_to_given_vec(..)\n",
    "entities_list = [wv['tokyo'], wv['vienna']]\n",
    "most_similar_to_given_vec(wv['tokyo'], entities_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#From name to node\n",
    "def toNode(name):\n",
    "    for node in nodeVectors.index:\n",
    "        if getPreferredTitle(node) == name:\n",
    "            return node\n",
    "    return None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#De-vectorise nodes and relations\n",
    "def deVectorise(vec):\n",
    "    if vec['source_type'] == 'node':\n",
    "        source = nodeVectors.copy()\n",
    "    else:\n",
    "        source = relationVectors.copy()\n",
    "    \n",
    "    source['dist'] = source.apply(lambda row: wv.cosine_similarities(list(vec['vec']), [list(row['vec'])])[0], axis = 1)\n",
    "    source = source.sort_values(by='dist', ascending=False)\n",
    "    \n",
    "    return source.iloc[0, source.columns.get_loc(vec['source_type'])]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#De-vecotrise nodes and relations generated by methods\n",
    "logger.info(f\"De-Vectorising...\")\n",
    "res['target'] = res.progress_apply(lambda row: deVectorise(row), axis = 1)\n",
    "res = res[['method', 'config', 'source_type', 'source', 'target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#TODO: for all appends: \n",
    "#https://stackoverflow.com/questions/50501787/python-pandas-user-warning-sorting-because-non-concatenation-axis-is-not-aligne\n",
    "\n",
    "#TODO: random seeds\n",
    "\n",
    "#TODO: check for TODOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Returns percent of ambiguity in the graph\n",
    "def calculateAmbiguity(fullVectors, nodeVectors, relationVectors):\n",
    "    #cFullVectors = fullVectors.copy()\n",
    "    #cNodeVectors = nodeVectors.copy()\n",
    "    #cRelationVectors = relationVectors.copy()\n",
    "    \n",
    "    #####for relations#####\n",
    "    \n",
    "    #group by relation type\n",
    "        #cross product group with itself\n",
    "        #apply(..) to calculate distance between both r_vec\n",
    "        #relation_distance = mean of all distances\n",
    "    #ambiguity = average weighted by count(all relation_distances)\n",
    "    #=> ambiguity values from -1..1 where 1 is the least ambiguous\n",
    "\n",
    "#     def helper1(grp):\n",
    "#         #returns the relation_distance for each group\n",
    "\n",
    "#         group = grp.copy().reset_index(drop=False)\n",
    "#         logger.info(f\"Group ({len(group)}):\\n{group['p'].iloc[0]}\\nColumns ({len(group.columns)}):\\n{group.columns}\\n\")\n",
    "#         logger.info(f\"{group}\")\n",
    "        \n",
    "#         out = group.apply(helper2, axis=1)\n",
    "#         logger.info(f\"Out:\\n{out}\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
    "        \n",
    "#         return np.mean(out)\n",
    "    \n",
    "#     def helper2(row):\n",
    "#         #returns the relation_distances for one group\n",
    "#         logger.info(f\"Row: {row['p']}\")\n",
    "        \n",
    "#         r_vecs = cFullVectors[cFullVectors['p'] == row['p']]['r_vec']\n",
    "#         logger.info(f\"r_vecs ({len(r_vecs)})\")\n",
    "        \n",
    "#         sims = wv.cosine_similarities(row['r_vec'], r_vecs)\n",
    "        \n",
    "#         #returns the relation_distance for one row\n",
    "#         out = np.mean(sims)\n",
    "        \n",
    "#         return out\n",
    "         \n",
    "        \n",
    "    #Remove unwanted entries\n",
    "    #cFullVectors = cFullVectors[cFullVectors['is_zero_vector_relation'] == False].dropna(subset=['r_vec']).reset_index(drop=True)\n",
    "    \n",
    "    #Broken because of https://github.com/pandas-dev/pandas/pull/29131\n",
    "    #relation_distances = fullVectors.groupby('p').apply(helper1)\n",
    "    \n",
    "    #Workaround\n",
    "    #relation_distances = cRelationVectors.apply(lambda row: helper1(fullVectors[fullVectors['p'] == row['relation']]), axis=1)\n",
    "    \n",
    "    #print(relation_distances)\n",
    "    \n",
    "    \n",
    "    ###2nd approach\n",
    "#     print(f\"Relation Vectors:\\n{cRelationVectors}\")\n",
    "    \n",
    "#     row = cRelationVectors.iloc[0]\n",
    "#     print(f\"Row: {row}\")\n",
    "#     print(f\"Row.vec:\\n{row['vec']}\")\n",
    "#     print(f\"r_vecs:\\n{list(cFullVectors[cFullVectors['p'] == row['relation']]['r_vec'])}\")\n",
    "#     print(f\"Result:\\n{wv.cosine_similarities(row['vec'], list(cFullVectors[cFullVectors['p'] == row['relation']]['r_vec']))}\")\n",
    "    \n",
    "    #cRelationVectors['mean_dist'] = cRelationVectors.apply(lambda row: np.mean(wv.cosine_similarities(row['vec'], list(cFullVectors[cFullVectors['p'] == row['relation']]['r_vec']))), axis=1)\n",
    "    \n",
    "    weights = relationVectors['total'] - relationVectors['lost'] - relationVectors['zero_vector']\n",
    "    relAmbig = np.average(relationVectors['mean_dist'], weights=weights)\n",
    "    \n",
    "    #####for nodes (improved)#####\n",
    "    \n",
    "    #for node in nodeVectors:\n",
    "        #node_estimate = average weighted by relations dist to mean(all connected nodes + their relation to node)\n",
    "    #ambiguity = average weighted by count of (distance(node_estimate, node))\n",
    "    #=> ambiguity values from -1..1 where 1 is the least ambiguous\n",
    "    \n",
    "    weights = nodeVectors['total']\n",
    "    nodeAmbig = np.average(nodeVectors['est_dist'], weights=weights)\n",
    "    \n",
    "    #bad quality of all nodes\n",
    "    #-> all relation vectors will have high average dist to mean\n",
    "    \n",
    "    #bad quality of all relations\n",
    "    #-> all relation vectors will have high average dist to mean\n",
    "    \n",
    "    #bad quality of some nodes\n",
    "    #-> node_estimate slightly wrong but better if many nodes\n",
    "    \n",
    "    #bad quality of some relations\n",
    "    #-> node_estimate very slightly wrong but better if many nodes and high relation dist to mean\n",
    "    \n",
    "    #perfect quality\n",
    "    #-> node_estimate = node and ambiguity = 1\n",
    "    \n",
    "    #do strongly connected nodes influence the outcome more? (they should)\n",
    "    #-> yes, they are included in more node_estimates\n",
    "    \n",
    "    #node and relation are included in each others calculations equally (=once) and only their means are used?\n",
    "    #-> yes\n",
    "    \n",
    "    \n",
    "    ##final composition & transformation\n",
    "    #mean of ambiguities of nodes and vectors, weight by nodes 2:1 relation\n",
    "    totAmbig = np.average([relAmbig, nodeAmbig], weights=[2, 1])\n",
    "    \n",
    "    #transform -1..1 where 1 is the least ambiguous to 0..1 where 1 is the most ambiguous\n",
    "    return 1-((1+totAmbig)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "data += [['tokyo', wv['berlin'], 1]]\n",
    "data += [['tokyo', wv['tokyo'], 2]]\n",
    "data += [['vienna', wv['vienna'], 3]]\n",
    "df2 = pd.DataFrame(data, columns=['node', 'vec', 'num'])\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "data += [['tokyo', wv['berlin'], 1]]\n",
    "data += [['tokyo', wv['tokyo'], 2]]\n",
    "data += [['vienna', wv['vienna'], 3]]\n",
    "df2 = pd.DataFrame(data, columns=['node', 'vec', 'num'])\n",
    "\n",
    "\n",
    "df2[['vec2', 'vec3']] = df2.apply(lambda row: pd.Series([toVector(row['node']), None]), axis=1)\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Test mean\n",
    "df2out = df2.groupby('node').apply(np.mean).reset_index()\n",
    "df2out['vec'] = df2.groupby('node')['vec'].apply(np.mean).reset_index()['vec']\n",
    "df2out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#NaN vs None problems\n",
    "print(f\"Python: {float('NaN') is None}\")\n",
    "print(f\"Numpy equal(..): {np.equal(float('NaN'), None)}\")\n",
    "print(f\"Numpy isnan(..): {np.isnan(float('NaN'))}\")\n",
    "print(f\"Pandas isnan(..): {pd.isnull(float('NaN'))}, {pd.isnull(None)}\") #Replace python checks with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NaN vs None problems\n",
    "print(f\"Python: {float('NaN') is None}\")\n",
    "print(f\"Numpy equal(..): {np.equal(float('NaN'), None)}\")\n",
    "print(f\"Numpy isnan(..): {np.isnan(float('NaN'))}\")\n",
    "print(f\"Pandas isnan(..): {pd.isnull(float('NaN'))}, {pd.isnull(None)}\") #Replace python checks with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random changes for testing\n",
    "config = {\n",
    "    'nodes': {\n",
    "        'random': [{'amount': {'num': 5}}]\n",
    "    },\n",
    "    'relations': {\n",
    "        'random': [{'amount': {'num': 2}}]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete negative for testing\n",
    "config = {\n",
    "    'nodes': {\n",
    "        'negative': [{'amount': {'perc': 1}}]\n",
    "    },\n",
    "    'relations': {\n",
    "        'negative': [{'amount': {'perc': 1}}]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closeness for testing\n",
    "config = {\n",
    "    'nodes': {\n",
    "        'closeness': [{'amount': {'num': 5},\n",
    "                 'param': {'closeness': 0.2}}]\n",
    "    },\n",
    "    'relations': {\n",
    "        'closeness': [{'amount': {'num': 5},\n",
    "                 'param': {'closeness': 0.2}}]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run imports and define functions, configure the desired result\n",
    "\n",
    "#Load graph and dictionary\n",
    "convertedGraph = convertGraph(g)\n",
    "fullVectors = vectorifyGraph(convertedGraph)\n",
    "relationVectors, lostRelations = generateRelationVectors(fullVectors)\n",
    "fullVectors = calculateNodeEstimates(fullVectors, relationVectors)\n",
    "nodeVectors, lostNodes = generateNodeVectors(fullVectors)\n",
    "\n",
    "#Check outputs before continuing\n",
    "\n",
    "#Calculate ambiguity before\n",
    "ambiguityBefore = calculateAmbiguity(fullVectors, nodeVectors, relationVectors)\n",
    "logger.info(f\"Ambiguity before: {ambiguityBefore}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The ambiguify-function returns vectors according to configured methods\n",
    "np.random.seed(0)\n",
    "res = ambiguify(config, nodeVectors, relationVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Insert new node into graph based on one random triple containing the source\n",
    "g2 = Graph()\n",
    "np.random.seed(0)\n",
    "additions = populateAdditions(res, g2)\n",
    "\n",
    "#Calculate ambiguity after:\n",
    "newFullVectors = fullVectors.copy()\n",
    "\n",
    "if len(additions) > 0:\n",
    "    logger.info(f\"Adding {len(additions)} additional triples\")\n",
    "    vectorisedAdditions = vectorifyGraph(additions)\n",
    "    newFullVectors = newFullVectors.append(vectorisedAdditions, ignore_index = True)\n",
    "\n",
    "newRelationVectors, newLostRelations = generateRelationVectors(newFullVectors)\n",
    "newFullVectors = calculateNodeEstimates(newFullVectors, newRelationVectors)\n",
    "newNodeVectors, newLostNodes = generateNodeVectors(newFullVectors)\n",
    "\n",
    "#Calculate ambiguity after\n",
    "ambiguityAfter = calculateAmbiguity(newFullVectors, newNodeVectors, newRelationVectors)\n",
    "logger.info(f\"Ambiguity after: {ambiguityAfter}\")\n",
    "\n",
    "logger.info(f\"Ambiguity difference: {ambiguityAfter-ambiguityBefore}\")\n",
    "\n",
    "\n",
    "\n",
    "#Save additions from nodes and relations\n",
    "logger.info(f\"Saving files\")\n",
    "f = open(\"additions.ttl\", \"wb\")\n",
    "f.write(g2.serialize(format='turtle'))\n",
    "f.close()\n",
    "\n",
    "#Save graph with additions\n",
    "g3 = g+g2\n",
    "f = open(\"appendedKG.ttl\", \"wb\")\n",
    "f.write(g3.serialize(format='turtle'))\n",
    "f.close()\n",
    "logger.info(f\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: for all appends: \n",
    "#https://stackoverflow.com/questions/50501787/python-pandas-user-warning-sorting-because-non-concatenation-axis-is-not-aligne\n",
    "\n",
    "#TODO: random seeds\n",
    "\n",
    "#TODO: check for TODOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns percent of ambiguity in the graph\n",
    "def calculateAmbiguity(fullVectors, nodeVectors, relationVectors):\n",
    "    #cFullVectors = fullVectors.copy()\n",
    "    #cNodeVectors = nodeVectors.copy()\n",
    "    #cRelationVectors = relationVectors.copy()\n",
    "    \n",
    "    #####for relations#####\n",
    "    \n",
    "    #group by relation type\n",
    "        #cross product group with itself\n",
    "        #apply(..) to calculate distance between both r_vec\n",
    "        #relation_distance = mean of all distances\n",
    "    #ambiguity = average weighted by count(all relation_distances)\n",
    "    #=> ambiguity values from -1..1 where 1 is the least ambiguous\n",
    "\n",
    "#     def helper1(grp):\n",
    "#         #returns the relation_distance for each group\n",
    "\n",
    "#         group = grp.copy().reset_index(drop=False)\n",
    "#         logger.info(f\"Group ({len(group)}):\\n{group['p'].iloc[0]}\\nColumns ({len(group.columns)}):\\n{group.columns}\\n\")\n",
    "#         logger.info(f\"{group}\")\n",
    "        \n",
    "#         out = group.apply(helper2, axis=1)\n",
    "#         logger.info(f\"Out:\\n{out}\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
    "        \n",
    "#         return np.mean(out)\n",
    "    \n",
    "#     def helper2(row):\n",
    "#         #returns the relation_distances for one group\n",
    "#         logger.info(f\"Row: {row['p']}\")\n",
    "        \n",
    "#         r_vecs = cFullVectors[cFullVectors['p'] == row['p']]['r_vec']\n",
    "#         logger.info(f\"r_vecs ({len(r_vecs)})\")\n",
    "        \n",
    "#         sims = wv.cosine_similarities(row['r_vec'], r_vecs)\n",
    "        \n",
    "#         #returns the relation_distance for one row\n",
    "#         out = np.mean(sims)\n",
    "        \n",
    "#         return out\n",
    "         \n",
    "        \n",
    "    #Remove unwanted entries\n",
    "    #cFullVectors = cFullVectors[cFullVectors['is_zero_vector_relation'] == False].dropna(subset=['r_vec']).reset_index(drop=True)\n",
    "    \n",
    "    #Broken because of https://github.com/pandas-dev/pandas/pull/29131\n",
    "    #relation_distances = fullVectors.groupby('p').apply(helper1)\n",
    "    \n",
    "    #Workaround\n",
    "    #relation_distances = cRelationVectors.apply(lambda row: helper1(fullVectors[fullVectors['p'] == row['relation']]), axis=1)\n",
    "    \n",
    "    #print(relation_distances)\n",
    "    \n",
    "    \n",
    "    ###2nd approach\n",
    "#     print(f\"Relation Vectors:\\n{cRelationVectors}\")\n",
    "    \n",
    "#     row = cRelationVectors.iloc[0]\n",
    "#     print(f\"Row: {row}\")\n",
    "#     print(f\"Row.vec:\\n{row['vec']}\")\n",
    "#     print(f\"r_vecs:\\n{list(cFullVectors[cFullVectors['p'] == row['relation']]['r_vec'])}\")\n",
    "#     print(f\"Result:\\n{wv.cosine_similarities(row['vec'], list(cFullVectors[cFullVectors['p'] == row['relation']]['r_vec']))}\")\n",
    "    \n",
    "    #cRelationVectors['mean_dist'] = cRelationVectors.apply(lambda row: np.mean(wv.cosine_similarities(row['vec'], list(cFullVectors[cFullVectors['p'] == row['relation']]['r_vec']))), axis=1)\n",
    "    \n",
    "    weights = relationVectors['total'] - relationVectors['lost'] - relationVectors['zero_vector']\n",
    "    relAmbig = np.average(relationVectors['mean_dist'], weights=weights)\n",
    "    \n",
    "    #####for nodes (improved)#####\n",
    "    \n",
    "    #for node in nodeVectors:\n",
    "        #node_estimate = average weighted by relations dist to mean(all connected nodes + their relation to node)\n",
    "    #ambiguity = average weighted by count of (distance(node_estimate, node))\n",
    "    #=> ambiguity values from -1..1 where 1 is the least ambiguous\n",
    "    \n",
    "    weights = nodeVectors['total']\n",
    "    nodeAmbig = np.average(nodeVectors['est_dist'], weights=weights)\n",
    "    \n",
    "    #bad quality of all nodes\n",
    "    #-> all relation vectors will have high average dist to mean\n",
    "    \n",
    "    #bad quality of all relations\n",
    "    #-> all relation vectors will have high average dist to mean\n",
    "    \n",
    "    #bad quality of some nodes\n",
    "    #-> node_estimate slightly wrong but better if many nodes\n",
    "    \n",
    "    #bad quality of some relations\n",
    "    #-> node_estimate very slightly wrong but better if many nodes and high relation dist to mean\n",
    "    \n",
    "    #perfect quality\n",
    "    #-> node_estimate = node and ambiguity = 1\n",
    "    \n",
    "    #do strongly connected nodes influence the outcome more? (they should)\n",
    "    #-> yes, they are included in more node_estimates\n",
    "    \n",
    "    #node and relation are included in each others calculations equally (=once) and only their means are used?\n",
    "    #-> yes\n",
    "    \n",
    "    \n",
    "    ##final composition & transformation\n",
    "    #mean of ambiguities of nodes and vectors, weight by nodes 2:1 relation\n",
    "    totAmbig = np.average([relAmbig, nodeAmbig], weights=[2, 1])\n",
    "    \n",
    "    #transform -1..1 where 1 is the least ambiguous to 0..1 where 1 is the most ambiguous\n",
    "    return 1-((1+totAmbig)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data += [['tokyo', wv['berlin'], 1]]\n",
    "data += [['tokyo', wv['tokyo'], 2]]\n",
    "data += [['vienna', wv['vienna'], 3]]\n",
    "df2 = pd.DataFrame(data, columns=['node', 'vec', 'num'])\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data += [['tokyo', wv['berlin'], 1]]\n",
    "data += [['tokyo', wv['tokyo'], 2]]\n",
    "data += [['vienna', wv['vienna'], 3]]\n",
    "df2 = pd.DataFrame(data, columns=['node', 'vec', 'num'])\n",
    "\n",
    "\n",
    "df2[['vec2', 'vec3']] = df2.apply(lambda row: pd.Series([toVector(row['node']), None]), axis=1)\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test mean\n",
    "df2out = df2.groupby('node').apply(np.mean).reset_index()\n",
    "df2out['vec'] = df2.groupby('node')['vec'].apply(np.mean).reset_index()['vec']\n",
    "df2out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NaN vs None problems\n",
    "print(f\"Python: {float('NaN') is None}\")\n",
    "print(f\"Numpy equal(..): {np.equal(float('NaN'), None)}\")\n",
    "print(f\"Numpy isnan(..): {np.isnan(float('NaN'))}\")\n",
    "print(f\"Pandas isnan(..): {pd.isnull(float('NaN'))}, {pd.isnull(None)}\") #Replace python checks with this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}