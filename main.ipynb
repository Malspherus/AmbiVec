{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% Imports\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import logging\n",
    "from rdflib import Graph, Namespace, Literal\n",
    "from rdflib.plugins.parsers import trig\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from rdflib import URIRef\n",
    "from rdflib.namespace import RDF\n",
    "from rdflib.namespace import RDFS\n",
    "from rdflib.namespace import SKOS\n",
    "from tqdm import tqdm\n",
    "from TqdmToLogger import TqdmToLogger\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "tqdm.pandas()\n",
    "tqdm_out = TqdmToLogger(logger, level=logging.INFO)\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO)\n",
    "#logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', None)\n",
    "#pd.reset_option('max_columns')\n",
    "#pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% Load/train dictionary\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gw1 = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wv = gw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% Load/train dictionary\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gw3 = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg = Graph()\n",
    "tg.parse(\"KGDemo.ttl\", format=\"ttl\")\n",
    "logger.info(f\"Loaded graph with {len(tg)} triples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sparql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sg = Graph()\n",
    "sg.parse(\"sparql\", format=\"ttl\")\n",
    "logger.info(f\"Loaded graph with {len(sg)} triples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "g = sg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getPreferredTitle(n, lang=\"en\"):\n",
    "    label = g.preferredLabel(n, lang=lang)\n",
    "\n",
    "    #if type(n) is not type(Literal(\"\")): #only labels should be of type literal\n",
    "    if label == []:\n",
    "        return n.rsplit('/', 1)[-1].replace('_', ' ').replace(',', '').lower() #TODO: replace \"()\"?\n",
    "    else:\n",
    "        return label[0][1].value.lower()\n",
    "    #else:\n",
    "    #    return None\n",
    "\n",
    "#Returns [vec, isMultipart, multipart-matched-%]\n",
    "def toVector(n):\n",
    "    title = getPreferredTitle(n)\n",
    "    #if title is None:\n",
    "    #    return [None, None, None]\n",
    "    \n",
    "    #In case of multiple words in title use mean of individual vectors\n",
    "    if \" \" in title:\n",
    "        subvecs = []\n",
    "        count = 0\n",
    "        hit = 0\n",
    "        for word in title.split(\" \"):\n",
    "            count += 1\n",
    "            try:\n",
    "                subvecs += [wv[word]]\n",
    "                hit += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "        if hit > 0:\n",
    "            return [sum(subvecs)/hit, True, hit/count]\n",
    "        else:\n",
    "            return [None, True, 0]\n",
    "    else:\n",
    "        try:\n",
    "            return [wv[title], False, False]\n",
    "        except KeyError:\n",
    "            return [None, False, False]\n",
    "\n",
    "#Methods to ambiguify nodes and relations\n",
    "def select(inp, obj={'perc': None, 'num': None}):\n",
    "    if len(obj) > 1:\n",
    "        raise TypeError(\"Please give exactly one of percentage or number\")\n",
    "        \n",
    "    for val in obj:\n",
    "        if val == 'perc':\n",
    "            return inp.sample(frac=obj[val], replace=True)\n",
    "        else:\n",
    "            return inp.sample(n=obj[val], replace=True)\n",
    "\n",
    "def ambiguify(config, nodeVectors, relationVectors):\n",
    "    out = pd.DataFrame()\n",
    "    for target in config: #can be \"nodes\" or \"relations\"\n",
    "        for method in config[target]: #matches the name of the method\n",
    "            for instance in config[target][method]: #once for every instance of the method config\n",
    "                for val in instance['amount']: #the amount of elements to be changed\n",
    "                    logger.info(f\"Ambiguifying {target} with {method} (parameters: {instance})\")\n",
    "                    if target == 'nodes':\n",
    "                        inp = nodeVectors\n",
    "                    else:\n",
    "                        inp = relationVectors\n",
    "                    \n",
    "                    selres = select(inp, instance['amount'])\n",
    "                    conf = instance.get('param', None)\n",
    "                    sourceColumn = target[0:-1]\n",
    "                    rep = pd.DataFrame()\n",
    "                    \n",
    "                    rep[['method', 'config', 'source_type', 'source', 'target']] = selres.progress_apply(lambda sel: pd.Series([\n",
    "                        method,\n",
    "                        str(instance),\n",
    "                        sourceColumn,\n",
    "                        sel[sourceColumn],\n",
    "                        methods[target][method](sel.copy(), inp.copy(), conf)[sourceColumn].iloc[0]\n",
    "                    ]), axis=1)\n",
    "                    \n",
    "                    out = out.append(rep, ignore_index=True)\n",
    "    return out\n",
    "\n",
    "#Modify triple and save as new\n",
    "def modTriple(row, g2, useObject = False, retry=False):\n",
    "    if row['source_type'] == 'relation':\n",
    "        logger.debug(\"Replacing r\")\n",
    "        fil = fullVectors['p'] == row['source']\n",
    "    else:\n",
    "        if useObject:\n",
    "            logger.debug(\"Replacing o\")\n",
    "            fil = fullVectors['o'] == row['source']\n",
    "        else:\n",
    "            logger.debug(\"Replacing s\")\n",
    "            fil = fullVectors['s'] == row['source']\n",
    "    \n",
    "    res = fullVectors[fil]\n",
    "\n",
    "    if len(res) == 0:\n",
    "        if not retry:\n",
    "            return modTriple(row, g2, useObject= not useObject, retry=True)\n",
    "        else:\n",
    "            logger.error(f\"Could not find original triple!\")\n",
    "    else:\n",
    "        rep = res.sample(n=1)\n",
    "\n",
    "        #add row to graph with changed content\n",
    "        if row['source_type'] == 'relation':\n",
    "            g2.add((rep['s'].iloc[0], row['target'], rep['o'].iloc[0]))\n",
    "            logger.debug(f\"{(rep['s'].iloc[0], row['target'], rep['o'].iloc[0])}\")\n",
    "            return (rep['s'].iloc[0], row['target'], rep['o'].iloc[0], rep['s'].iloc[0], rep['p'].iloc[0], rep['o'].iloc[0])\n",
    "        else:\n",
    "            if useObject:\n",
    "                g2.add((rep['s'].iloc[0], rep['p'].iloc[0], row['target']))\n",
    "                logger.debug(f\"{(rep['s'].iloc[0], rep['p'].iloc[0], row['target'])}\")\n",
    "                return (rep['s'].iloc[0], rep['p'].iloc[0], row['target'], rep['s'].iloc[0], rep['p'].iloc[0], rep['o'].iloc[0])\n",
    "            else:\n",
    "                g2.add((row['target'], rep['p'].iloc[0], rep['o'].iloc[0]))\n",
    "                logger.debug(f\"{(row['target'], rep['p'].iloc[0], rep['o'].iloc[0])}\")\n",
    "                return (row['target'], rep['p'].iloc[0], rep['o'].iloc[0], rep['s'].iloc[0], rep['p'].iloc[0], rep['o'].iloc[0])\n",
    "        \n",
    "def populateAdditions(res, g2):\n",
    "    logger.info(f\"Populatig graph\")\n",
    "    \n",
    "    out = pd.DataFrame()\n",
    "    out[['s', 'p', 'o', 's_orig', 'p_orig', 'o_orig']] = res.progress_apply(lambda row: pd.Series(modTriple(row, g2, useObject=(np.random.random() >= 0.5))), axis=1)\n",
    "    \n",
    "    res[['s_orig', 'p_orig', 'o_orig']] = out[['s_orig', 'p_orig', 'o_orig']]\n",
    "    out = out.drop(columns={'s_orig', 'p_orig', 'o_orig'})\n",
    "    \n",
    "    return out\n",
    "\n",
    "#Returns percent of ambiguity in the graph\n",
    "def calculateAmbiguity(fullVectors, nodeVectors, relationVectors):\n",
    "    #for relations\n",
    "    weights = relationVectors['total'] - relationVectors['lost'] - relationVectors['zero_vector']\n",
    "    relAmbig = np.average(relationVectors['mean_dist'], weights=weights)\n",
    "    \n",
    "    #for nodes\n",
    "    weights = nodeVectors['total']\n",
    "    nodeAmbig = np.average(nodeVectors['est_dist'], weights=weights)\n",
    "    \n",
    "    #mean of ambiguities of nodes and vectors, weight by nodes 2:1 relation\n",
    "    totAmbig = np.average([relAmbig, nodeAmbig], weights=[2, 1])\n",
    "    \n",
    "    #transform -1..1 where 1 is the least ambiguous to 0..1 where 1 is the most ambiguous\n",
    "    return 1-((1+totAmbig)/2)\n",
    "\n",
    "#Calculates the distance-severity of ambiguities\n",
    "def getDistSeverity(sourceType, source, target):\n",
    "    if sourceType == 'node':\n",
    "        inp = nodeVectors.copy()\n",
    "    else:\n",
    "        inp = relationVectors.copy()\n",
    "    \n",
    "    sourceEntry = inp[inp[sourceType] == URIRef(source)]\n",
    "\n",
    "    inp['dist'] = inp.apply(lambda row: wv.cosine_similarities(list(sourceEntry['vec'])[0], [list(row['vec'])])[0], axis = 1)\n",
    "    inp = inp.sort_values(by=['dist'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    targetEntry = inp[inp[sourceType] == URIRef(target)]\n",
    "        \n",
    "    return targetEntry.index[0]\n",
    "\n",
    "#Calculates the closeness-severity of ambiguities\n",
    "def getClosenessSeverity(sourceType, source, target):\n",
    "    #print(\"\\n\\n\\n\")\n",
    "    if sourceType == 'node':\n",
    "        inp = nodeVectors.copy()\n",
    "        #print(\"inp is node\")\n",
    "    else:\n",
    "        inp = relationVectors.copy()\n",
    "        #print(\"inp is rel\")\n",
    "    \n",
    "    sourceEntry = inp[inp[sourceType] == URIRef(source)]\n",
    "\n",
    "    inp['dist'] = inp.apply(lambda row: wv.cosine_similarities(list(sourceEntry['vec'])[0], [list(row['vec'])])[0], axis = 1)\n",
    "    inp = inp.sort_values(by=['dist'], ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    targetEntry = inp[inp[sourceType] == URIRef(target)]\n",
    "    \n",
    "    if len(targetEntry) == 0:\n",
    "        logger.warning(\"Defect entry due to library bug, skipping with closeness value None\")\n",
    "        return None\n",
    "\n",
    "    return np.absolute([targetEntry['dist'].iloc[0]])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Setup methods\n",
    "def rand(inp, source, conf):\n",
    "    return source.sample(n=1)\n",
    "\n",
    "#Find result with specific distance\n",
    "def dist(inp, source, conf):\n",
    "    if conf == None:\n",
    "        dist = 1\n",
    "    else:\n",
    "        dist = conf.get('dist', 1)\n",
    "        \n",
    "    dist = min(max(dist, 0), len(source.index)-1)\n",
    "    \n",
    "    #use pandas to get top-n, if dist is same move inp to the top\n",
    "    source[['dist', 'isInp']] = source.apply(lambda row: pd.Series([wv.cosine_similarities(list(inp['vec']), [list(row['vec'])])[0], inp[0] == row[0]]), axis = 1)\n",
    "    source = source.sort_values(by=['dist', 'isInp'], ascending=False)\n",
    "    \n",
    "    logger.debug(f\"source:\\n{source}\")\n",
    "    \n",
    "    logger.debug(f\"dist: {dist}\")\n",
    "    logger.debug(f\"choice:\\n{source.iloc[[dist]]}\")\n",
    "\n",
    "    return source.iloc[[dist]].drop(['dist', 'isInp'], axis=1)\n",
    "\n",
    "#Find result with specific closeness\n",
    "def closeness(inp, source, conf):\n",
    "    if conf == None:\n",
    "        closeness = 1\n",
    "    else:\n",
    "        closeness = conf.get('closeness', 1)\n",
    "        \n",
    "    closeness = min(max(closeness, 0), 2) #0 is equal to the input, 2 is its inverse\n",
    "    \n",
    "    #use pandas to get dists\n",
    "    source[['dist', 'isInp']] = source.apply(lambda row: pd.Series([wv.cosine_similarities(list(inp['vec']), [list(row['vec'])])[0], inp[0] == row[0]]), axis = 1)\n",
    "    resIndex = source['dist'].add(closeness-1).abs().idxmin()\n",
    "    \n",
    "    return source.iloc[[resIndex]].drop(['dist', 'isInp'], axis=1)\n",
    "\n",
    "#Find result closest to inverse input vector\n",
    "def negative(inp, source, conf):\n",
    "    return closeness(inp, source, {'closeness': 2})\n",
    "\n",
    "methods = {\n",
    "    'nodes':{\n",
    "        'random': rand,\n",
    "        'dist': dist,\n",
    "        'closeness': closeness,\n",
    "        'negative': negative\n",
    "    },\n",
    "    'relations': {\n",
    "        'random': rand,\n",
    "        'dist': dist,\n",
    "        'closeness': closeness,\n",
    "        'negative': negative\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertGraph(g):\n",
    "    logger.info(f\"Converting graph\")\n",
    "    length = 0\n",
    "    for s, p, o in g.triples((None, None, None)):\n",
    "        length += 1\n",
    "\n",
    "    fullVectors = []\n",
    "    for s, p, o in tqdm(g.triples((None, None, None)), total=length, file=tqdm_out, mininterval=2):\n",
    "        fullVectors += [[s, p, o]]\n",
    "\n",
    "    return pd.DataFrame(data=fullVectors, columns=['s', 'p', 'o'])\n",
    "\n",
    "def vectorifyGraph(fullVectors):\n",
    "    logger.info(f\"Vecotrifying graph\")\n",
    "\n",
    "    logger.info('Subject vectors')\n",
    "    fullVectors[['s_vec', 's_is_multipart', 's_multipart_%']] = fullVectors.progress_apply(lambda row: pd.Series(toVector(row['s'])), axis=1)\n",
    "\n",
    "    logger.info('Object vectors')\n",
    "    fullVectors[['o_vec', 'o_is_multipart', 'o_multipart_%']] = fullVectors.progress_apply(lambda row: pd.Series(toVector(row['o'])), axis=1)\n",
    "\n",
    "    logger.info('Relation vectors')\n",
    "    fullVectors[['r_vec', 'is_zero_vector_relation']] = fullVectors.progress_apply(lambda row: pd.Series([\n",
    "        row['o_vec']-row['s_vec'] if row['o_vec'] is not None and row['s_vec'] is not None else None,\n",
    "        np.array_equal(row['o_vec']-row['s_vec'], [0]*len(row['o_vec'])) if row['o_vec'] is not None and row['s_vec'] is not None else False\n",
    "    ]), axis=1)\n",
    "    \n",
    "    return fullVectors\n",
    "\n",
    "def generateRelationVectors(fullVectors):\n",
    "    logger.info(f\"Generating relationVectors\")\n",
    "    \n",
    "    logger.debug(f\"Grouping\")\n",
    "    relationVectors = fullVectors.groupby('p')['r_vec'].apply(np.mean).reset_index().rename(columns={'p': 'relation', 'r_vec': 'vec'})\n",
    "\n",
    "    logger.debug(f\"Calculating total, lost, zero_vector and quality\")\n",
    "    relationVectors['total'] = fullVectors.groupby('p')['p'].count().reset_index(drop=True)\n",
    "    relationVectors['lost'] = fullVectors.groupby('p')['r_vec'].apply(lambda x: x.isnull().sum()).reset_index(drop=True)\n",
    "    relationVectors['zero_vector'] = fullVectors.groupby('p')['is_zero_vector_relation'].sum().astype(int).reset_index(drop=True)\n",
    "    relationVectors['quality'] = relationVectors.apply(lambda row: 1-(row['lost']+row['zero_vector'])/row['total'], axis=1)\n",
    "\n",
    "    #TODO: labels are counted as lost nodes\n",
    "    #Split into relationVectors and lostRelations\n",
    "    logger.debug(f\"Splitting into relationVectors and lostRelations\")\n",
    "    lostRelations = relationVectors[np.bitwise_or(\n",
    "        relationVectors['vec'].isnull(),\n",
    "        relationVectors['total']-relationVectors['lost']-relationVectors['zero_vector'] == 0\n",
    "    )].reset_index(drop=True).drop(columns=['vec', 'quality'])\n",
    "    relationVectors = relationVectors[np.bitwise_and(\n",
    "        relationVectors['vec'].notnull(),\n",
    "        relationVectors['total']-relationVectors['lost']-relationVectors['zero_vector'] != 0\n",
    "    )].reset_index(drop=True)\n",
    "    \n",
    "    #Min/max/average distance of every full vector of this relation type to mean vector\n",
    "    logger.debug(f\"Calculating min, max and average distances\")\n",
    "    def helper_dist(row):\n",
    "        vectors = fullVectors[fullVectors['p'] == row['relation']]\n",
    "\n",
    "        #filter out None and zero-vector\n",
    "        vectors = vectors[vectors['is_zero_vector_relation'] == False]\n",
    "        vectors = vectors['r_vec'].dropna().reset_index(drop=True)\n",
    "\n",
    "        sims = wv.cosine_similarities(row['vec'], list(vectors))\n",
    "        return [np.min(sims), np.max(sims), np.mean(sims)]\n",
    "\n",
    "    relationVectors[['min_dist', 'max_dist', 'mean_dist']] = relationVectors.apply(lambda row: pd.Series(helper_dist(row)), axis=1)\n",
    "    logger.info(\"Done\")\n",
    "    \n",
    "    return relationVectors, lostRelations\n",
    "\n",
    "def calculateNodeEstimates(fullVectors, relationVectors):\n",
    "    logger.info(f\"Calculating node estimates\")\n",
    "    \n",
    "    def helper(row, op, same, other):\n",
    "        #Select the relation vector if there is one\n",
    "        relVecs = relationVectors[relationVectors['relation'] == row['p']]\n",
    "        if len(relVecs) > 0:\n",
    "            rVec = relVecs['vec'].iloc[0]\n",
    "        else:\n",
    "            rVec = None\n",
    "        \n",
    "        #Calculate the estimate\n",
    "        if row[other] is not None and rVec is not None:\n",
    "            est = op(row[other], rVec)\n",
    "        else:\n",
    "            est = None\n",
    "        \n",
    "        #Calculate the distance\n",
    "        if est is not None and row[same] is not None:\n",
    "            dist = wv.cosine_similarities(row[same], [est])[0]\n",
    "        else:\n",
    "            dist = None\n",
    "        \n",
    "        return pd.Series([est, dist], dtype='object')\n",
    "\n",
    "    logger.info(f\"Subject estimates\")\n",
    "    fullVectors[['s_est', 's_est_dist']] = fullVectors.progress_apply(helper, args=[np.subtract, 's_vec', 'o_vec'], axis=1)\n",
    "    \n",
    "    logger.info(f\"Object estimates\")\n",
    "    fullVectors[['o_est', 'o_est_dist']] = fullVectors.progress_apply(helper, args=[np.add, 'o_vec', 's_vec'], axis=1)\n",
    "    \n",
    "    #Workaround for readability as pandas is equaling NaN and None\n",
    "    fullVectors = fullVectors.astype({'s_est': 'object', 's_est_dist': 'object', 'o_est': 'object', 'o_est_dist': 'object'})\n",
    "    fullVectors.loc[fullVectors['s_est'].isna(), 's_est'] = None\n",
    "    fullVectors.loc[fullVectors['s_est_dist'].isna(), 's_est_dist'] = None\n",
    "    fullVectors.loc[fullVectors['o_est'].isna(), 'o_est'] = None\n",
    "    fullVectors.loc[fullVectors['o_est_dist'].isna(), 'o_est_dist'] = None\n",
    "    \n",
    "    return fullVectors\n",
    "\n",
    "def generateNodeVectors(fullVectors):\n",
    "    logger.info(f\"Generating nodeVectors\")\n",
    "    \n",
    "    #Rename and merge\n",
    "    logger.debug(f\"Renaming and merging\")\n",
    "    subjectVectors = fullVectors[['s', 's_vec', 's_is_multipart', 's_multipart_%', 's_est', 's_est_dist']].rename(columns={'s': 'node',\n",
    "                                                                                                    's_vec': 'vec',\n",
    "                                                                                                    's_is_multipart': 'is_multipart',\n",
    "                                                                                                    's_multipart_%': 'multipart_%',\n",
    "                                                                                                    's_est': 'est',\n",
    "                                                                                                    's_est_dist': 'est_dist'})\n",
    "    objectVectors = fullVectors[['o', 'o_vec', 'o_is_multipart', 'o_multipart_%', 'o_est', 'o_est_dist']].rename(columns={'o': 'node',\n",
    "                                                                                                   'o_vec': 'vec',\n",
    "                                                                                                   'o_is_multipart': 'is_multipart',\n",
    "                                                                                                   'o_multipart_%': 'multipart_%',\n",
    "                                                                                                   'o_est': 'est',\n",
    "                                                                                                   'o_est_dist': 'est_dist'})\n",
    "    nodeVectors = pd.concat([subjectVectors, objectVectors], ignore_index=True)\n",
    "    \n",
    "    #Remove duplicates\n",
    "    logger.debug(f\"Grouping\")\n",
    "    nodeGroup = nodeVectors.groupby('node')\n",
    "\n",
    "    logger.debug(f\"Using first for vector\")\n",
    "    #nodeVectors = nodeGroup.first().reset_index() #this is really slow\n",
    "    nodeVectors = nodeGroup.head(1).reset_index(drop=True)\n",
    "    \n",
    "    logger.debug(f\"Calculating totals\")\n",
    "    nodeVectors['total'] = nodeGroup.size().reset_index(drop=True)\n",
    "    \n",
    "    logger.debug(f\"Calculating estimates\")\n",
    "    nodeVectors['est'] = nodeGroup['est'].apply(np.mean).reset_index(drop=True)\n",
    "    \n",
    "    #Workaround as pandas is equaling NaN and None\n",
    "    nodeVectors = nodeVectors.astype({'est': 'object', 'vec': 'object'})\n",
    "    nodeVectors.loc[nodeVectors['est'].isna(), 'est'] = None\n",
    "    nodeVectors.loc[nodeVectors['vec'].isna(), 'vec'] = None\n",
    "    \n",
    "    logger.debug(f\"Calculating estimate distances\")\n",
    "    nodeVectors['est_dist'] = nodeVectors.apply(lambda row: wv.cosine_similarities(list(row['est']), [list(row['vec'])])[0] if row['est'] is not None and row['vec'] is not None else None, axis=1)\n",
    "    \n",
    "    logger.debug(f\"Calculating mean/min/max of distances\")\n",
    "    nodeVectors['mean_est_dist'] = nodeGroup['est_dist'].apply(np.mean).reset_index(drop=True)\n",
    "    nodeVectors['min_est_dist'] = nodeGroup['est_dist'].apply(np.min).reset_index(drop=True)\n",
    "    nodeVectors['max_est_dist'] = nodeGroup['est_dist'].apply(np.max).reset_index(drop=True)\n",
    "\n",
    "    #Split into nodeVectors and lostNodes\n",
    "    logger.debug(f\"Splitting into nodeVectors and lostNodes\")\n",
    "    lostNodes = nodeVectors[nodeVectors['vec'].isnull()].reset_index(drop=True).drop(columns=['vec', 'est_dist', 'mean_est_dist', 'min_est_dist', 'max_est_dist'])\n",
    "    \n",
    "    nodeVectors = nodeVectors.dropna().reset_index(drop=True)\n",
    "    logger.info(\"Done\")\n",
    "\n",
    "    return nodeVectors, lostNodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertedGraph = convertGraph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fullVectors = vectorifyGraph(convertedGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationVectors, lostRelations = generateRelationVectors(fullVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullVectors = calculateNodeEstimates(fullVectors, relationVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeVectors, lostNodes = generateNodeVectors(fullVectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & load dataframes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "fullVectors.to_pickle(\"fullVectors.plk\")\n",
    "nodeVectors.to_pickle(\"nodeVectors.plk\")\n",
    "lostNodes.to_pickle(\"lostNodes.plk\")\n",
    "relationVectors.to_pickle(\"relationVectors.plk\")\n",
    "lostRelations.to_pickle(\"lostRelations.plk\")\n",
    "changes.to_pickle(\"changes.plk\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "fullVectors = pd.read_pickle(\"fullVectors.plk\")\n",
    "nodeVectors = pd.read_pickle(\"nodeVectors.plk\")\n",
    "lostNodes = pd.read_pickle(\"lostNodes.plk\")\n",
    "relationVectors = pd.read_pickle(\"relationVectors.plk\")\n",
    "lostRelations = pd.read_pickle(\"lostRelations.plk\")\n",
    "changes = pd.read_pickle(\"changes.plk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View calculated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length: {len(fullVectors)}\")\n",
    "fullVectors.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show rows where the relation vector was lost\n",
    "fullVectors[fullVectors['r_vec'].isnull()].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show rows where the relation vector is a zero-vector\n",
    "fullVectors[fullVectors['is_zero_vector_relation'] == True].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length: {len(nodeVectors)}\")\n",
    "nodeVectors.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Length: {len(lostNodes)}\")\n",
    "lostNodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Relation Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length: {len(relationVectors)}\")\n",
    "relationVectors.head(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Length: {len(lostRelations)}\")\n",
    "lostRelations.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual calculations for verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Full vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multipart\n",
    "print(nodeVectors['is_multipart'].iloc[20])\n",
    "print(nodeVectors['multipart_%'].iloc[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zero-Vector\n",
    "print(fullVectors['is_zero_vector_relation'].iloc[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lost relation\n",
    "print(fullVectors['r_vec'].iloc[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Estimate\n",
    "print(fullVectors['s_est'].iloc[18], fullVectors['s_est_dist'].iloc[18])\n",
    "\n",
    "#print(fullVectors['s_est'].iloc[0])\n",
    "\n",
    "#Calculate estimate (minus means arrow from R to L for relationVector calculation)\n",
    "est = fullVectors['o_vec'].iloc[0]-relationVectors['vec'].iloc[2]\n",
    "#print(est)\n",
    "\n",
    "print(wv.cosine_similarities(fullVectors['s_est'].iloc[0], [est])[0])\n",
    "\n",
    "print(fullVectors['s_est_dist'].iloc[0])\n",
    "print(wv.cosine_similarities(fullVectors['s_vec'].iloc[0], [est])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lost relation\n",
    "print(lostRelations['lost'].iloc[0])\n",
    "print(lostRelations['zero_vector'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quality\n",
    "print(relationVectors['quality'].iloc[1])\n",
    "print((relationVectors['total'].iloc[1]-relationVectors['lost'].iloc[1]-relationVectors['zero_vector'].iloc[1])/relationVectors['total'].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Unlabeled\n",
    "np.array_equal(fullVectors['o_vec'].iloc[32], wv['australia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lost node\n",
    "lostNodes.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Node with the maximal distance to 'berlin'\n",
    "nodeVectors.iloc[np.argmin(wv.cosine_similarities(wv['berlin'], list(nodeVectors['vec'])))]['node']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Dist method\n",
    "\n",
    "#Same\n",
    "print(nodeVectors['node'].iloc[0], dist(nodeVectors.iloc[0].copy(), nodeVectors.copy(), {'dist': 0})['node'].iloc[0])\n",
    "\n",
    "#Inverse\n",
    "print(nodeVectors['node'].iloc[0], dist(nodeVectors.iloc[0].copy(), nodeVectors.copy(), {'dist': 100})['node'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closeness method\n",
    "\n",
    "#Same\n",
    "print(nodeVectors['node'].iloc[0], closeness(nodeVectors.iloc[0].copy(), nodeVectors.copy(), {'closeness': 0})['node'].iloc[0])\n",
    "\n",
    "#Inverse\n",
    "print(nodeVectors['node'].iloc[0], closeness(nodeVectors.iloc[0].copy(), nodeVectors.copy(), {'closeness': 2})['node'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ambiguity\n",
    "print(\"0.1191733359168643\")\n",
    "\n",
    "#for relations\n",
    "weights = relationVectors['total'] - relationVectors['lost'] - relationVectors['zero_vector']\n",
    "relAmbig = np.average(relationVectors['mean_dist'], weights=weights)\n",
    "\n",
    "print(np.array_equal(weights, [6, 5, 7]))\n",
    "print(relAmbig)\n",
    "print((relationVectors['mean_dist'].iloc[0]*6+relationVectors['mean_dist'].iloc[1]*5+relationVectors['mean_dist'].iloc[2]*7)/(6+5+7))\n",
    "\n",
    "#for nodes\n",
    "weights = nodeVectors['total']\n",
    "nodeAmbig = np.average(nodeVectors['est_dist'], weights=weights)\n",
    "\n",
    "#mean of ambiguities of nodes and vectors, weight by nodes 2:1 relation\n",
    "totAmbig = np.average([relAmbig, nodeAmbig], weights=[2, 1])\n",
    "\n",
    "#transform -1..1 where 1 is the least ambiguous to 0..1 where 1 is the most ambiguous\n",
    "print(1-((1+totAmbig)/2))\n",
    "\n",
    "print(1-((1+(-1))/2))\n",
    "print(1-((1+(1))/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate different amounts of ambiguity\n",
    "config = {\n",
    "    'nodes': {\n",
    "        'random': [{'amount': {'num': 5}}],\n",
    "        'dist': [{\n",
    "                    'amount': {'num': 5},\n",
    "                     'param': {'dist': 0}\n",
    "                 }, {\n",
    "                     'amount': {'num': 5},\n",
    "                     'param': {'dist': 1}\n",
    "                 }, {\n",
    "                     'amount': {'num': 5},\n",
    "                     'param': {'dist': 2}\n",
    "                 }, {\n",
    "                     'amount': {'num': 5},\n",
    "                     'param': {'dist': 3}\n",
    "                 }, {\n",
    "                     'amount': {'num': 5},\n",
    "                     'param': {'dist': 4}\n",
    "                 }]\n",
    "    },\n",
    "    'relations': {\n",
    "        'random': [{'amount': {'num': 5}}],\n",
    "        'dist': [{\n",
    "                    'amount': {'num': 5},\n",
    "                     'param': {'dist': 0}\n",
    "                 }, {\n",
    "                     'amount': {'num': 5},\n",
    "                     'param': {'dist': 1}\n",
    "                 }, {\n",
    "                     'amount': {'num': 5},\n",
    "                     'param': {'dist': 2}\n",
    "                 }, {\n",
    "                     'amount': {'num': 5},\n",
    "                     'param': {'dist': 3}\n",
    "                 }, {\n",
    "                     'amount': {'num': 5},\n",
    "                     'param': {'dist': 4}\n",
    "                 }]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Run imports and define functions, configure the desired result\n",
    "\n",
    "#Load graph and dictionary, you can skip this if you already ran it\n",
    "convertedGraph = convertGraph(g)\n",
    "fullVectors = vectorifyGraph(convertedGraph)\n",
    "relationVectors, lostRelations = generateRelationVectors(fullVectors)\n",
    "fullVectors = calculateNodeEstimates(fullVectors, relationVectors)\n",
    "nodeVectors, lostNodes = generateNodeVectors(fullVectors)\n",
    "\n",
    "#Check outputs before continuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#The ambiguify-function returns vectors according to configured methods\n",
    "np.random.seed(0)\n",
    "changes = ambiguify(config, nodeVectors, relationVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert new node into graph based on one random triple containing the source\n",
    "g2 = Graph()\n",
    "np.random.seed(0)\n",
    "additions = populateAdditions(changes, g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Save additions from nodes and relations\n",
    "logger.info(f\"Saving files\")\n",
    "\n",
    "f = open(\"additions.ttl\", \"wb\")\n",
    "f.write(g2.serialize(format='turtle'))\n",
    "f.close()\n",
    "\n",
    "#Save graph with additions\n",
    "g3 = g+g2\n",
    "f = open(\"appendedKG.ttl\", \"wb\")\n",
    "f.write(g3.serialize(format='turtle'))\n",
    "f.close()\n",
    "\n",
    "logger.info(f\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate ambiguity values\n",
    "#Calculate ambiguity before\n",
    "ambiguityBefore = calculateAmbiguity(fullVectors, nodeVectors, relationVectors)\n",
    "logger.info(f\"Ambiguity before: {ambiguityBefore}\")\n",
    "\n",
    "#Prepare data to calculate ambiguity after\n",
    "newFullVectors = fullVectors.copy()\n",
    "\n",
    "if len(additions) > 0:\n",
    "    logger.info(f\"Adding {len(additions)} additional triples\")\n",
    "    vectorisedAdditions = vectorifyGraph(additions)\n",
    "    newFullVectors = newFullVectors.append(vectorisedAdditions, ignore_index = True)\n",
    "\n",
    "newRelationVectors, newLostRelations = generateRelationVectors(newFullVectors)\n",
    "newFullVectors = calculateNodeEstimates(newFullVectors, newRelationVectors)\n",
    "newNodeVectors, newLostNodes = generateNodeVectors(newFullVectors)\n",
    "\n",
    "#Calculate ambiguity after\n",
    "ambiguityAfter = calculateAmbiguity(newFullVectors, newNodeVectors, newRelationVectors)\n",
    "logger.info(f\"Ambiguity after: {ambiguityAfter}\")\n",
    "\n",
    "logger.info(f\"Ambiguity difference: {ambiguityAfter-ambiguityBefore}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export for AMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate different severities for same original triple\n",
    "\n",
    "def ambiguify4amt(config, nodeVectors, relationVectors):\n",
    "    out = pd.DataFrame()\n",
    "    for target in config: #can be \"nodes\" or \"relations\"\n",
    "        if target == 'nodes':\n",
    "            inp = nodeVectors\n",
    "            #selres = nodeSel\n",
    "        else:\n",
    "            inp = relationVectors\n",
    "            #selres = relSel\n",
    "            \n",
    "        selres = inp.sample(n=5, replace=True) # hardcoded for evaluation\n",
    "        #print(selres.head(100))\n",
    "        \n",
    "        for method in config[target]: #matches the name of the method\n",
    "            for instance in config[target][method]: #once for every instance of the method config\n",
    "\n",
    "                logger.debug(f\"Selres: {selres}\")\n",
    "                \n",
    "                for val in instance['amount']: #the amount of elements to be changed\n",
    "                    logger.info(f\"Ambiguifying {target} with {method} (parameters: {instance})\")\n",
    "                    \n",
    "                    conf = instance.get('param', None)\n",
    "                    sourceColumn = target[0:-1]\n",
    "                    rep = pd.DataFrame()\n",
    "                    \n",
    "                    rep[['method', 'config', 'source_type', 'source', 'target']] = selres.progress_apply(lambda sel: pd.Series([\n",
    "                        method,\n",
    "                        str(instance),\n",
    "                        sourceColumn,\n",
    "                        sel[sourceColumn],\n",
    "                        methods[target][method](sel.copy(), inp.copy(), conf)[sourceColumn].iloc[0]\n",
    "                    ]), axis=1)\n",
    "                    \n",
    "                    out = out.append(rep, ignore_index=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "changes = ambiguify4amt(config, nodeVectors, relationVectors)\n",
    "\n",
    "g2 = Graph()\n",
    "np.random.seed(0)\n",
    "additions = populateAdditions(changes, g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change format\n",
    "amt = pd.concat([changes, additions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute names\n",
    "def getPreferredTitle4amt(n, lang=\"en\"):\n",
    "    label = g.preferredLabel(n, lang=lang)\n",
    "\n",
    "    #if type(n) is not type(Literal(\"\")): #only labels should be of type literal\n",
    "    if label == []:\n",
    "        return n.rsplit('/', 1)[-1].replace('_', ' ').replace(',', '')\n",
    "    else:\n",
    "        return label[0][1].value\n",
    "\n",
    "\n",
    "amt['s_orig_name'] = amt['s_orig'].apply(getPreferredTitle4amt)\n",
    "amt['p_orig_name'] = amt['p_orig'].apply(getPreferredTitle4amt)\n",
    "amt['o_orig_name'] = amt['o_orig'].apply(getPreferredTitle4amt)\n",
    "\n",
    "amt['s_name'] = amt['s'].apply(getPreferredTitle4amt)\n",
    "amt['p_name'] = amt['p'].apply(getPreferredTitle4amt)\n",
    "amt['o_name'] = amt['o'].apply(getPreferredTitle4amt)\n",
    "\n",
    "#Compute verification code\n",
    "def ver4amt(row):\n",
    "    num = np.array([\n",
    "        np.random.randint(np.min([len(row['s_name']), 5])-1),\n",
    "        np.random.randint(np.min([len(row['p_name']), 5])-1),\n",
    "        np.random.randint(np.min([len(row['o_name']), 5])-1)\n",
    "    ])\n",
    "    \n",
    "    code = (row['s_name'][num[0]]+row['p_name'][num[1]]+row['o_name'][num[2]]).replace(' ', '_')\n",
    "\n",
    "    #print(row[['s_name', 'p_name', 'o_name']], num+1, code)\n",
    "    return ('-'.join(str(x) for x in (num+1)), code)\n",
    "\n",
    "np.random.seed(0)\n",
    "amt[['ver_num', 'ver_code']] = amt.apply(lambda row: pd.Series(ver4amt(row)), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amt.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes.to_pickle(\"changes.plk\")\n",
    "additions.to_pickle(\"additions.plk\")\n",
    "amt.to_pickle(\"amt.plk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amt.to_csv(\"amt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process AMT results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amtres = pd.read_csv(\"Batch_305165_batch_results.csv\") #pilot 1\n",
    "#amtres = pd.read_csv(\"Batch_305480_batch_results.csv\") #pilot 2\n",
    "amtres = pd.read_csv(\"Batch_4310201_batch_results.csv\") #paid AMT questionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract and format data for plots\n",
    "\n",
    "amtres['severity'] = amtres.apply(lambda row: getDistSeverity(row['Input.source_type'], row['Input.source'], row['Input.target'])\n",
    "                                  if row['Input.method'] == 'random'\n",
    "                                  else (json.loads(row['Input.config'].replace(\"'\", '\"'))['param']['dist']\n",
    "                                        if row['Input.method'] == 'dist'\n",
    "                                        else -1)\n",
    "                                  , axis = 1)\n",
    "\n",
    "#Note: this data may not be available in the pilots\n",
    "amtres['Answer.mistakeSource'] = amtres.apply(lambda row: 'artificial'\n",
    "                                  if row['Answer.mistakeSource.artificial'] == True\n",
    "                                  else ( 'human'\n",
    "                                        if row['Answer.mistakeSource.human'] == True\n",
    "                                        else 'correct')\n",
    "                                  , axis = 1)\n",
    "\n",
    "amtres['closeness'] = amtres.apply(lambda row: getClosenessSeverity(row['Input.source_type'], row['Input.source'], row['Input.target']), axis = 1)\n",
    "\n",
    "#data = amtres[['Answer.stars', 'WorkTimeInSeconds', 'severity']]#.sort_values(['Answer.stars', 'WorkTimeInSeconds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set accept/reject for amt answers and export\n",
    "\n",
    "#Reject tasktime outliers\n",
    "Q1 = amtres.quantile(0.25)\n",
    "Q3 = amtres.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "quant = (amtres < (Q1 - 3 * IQR)) | (amtres > (Q3 + 3 * IQR))\n",
    "\n",
    "amtres['Reject'] = amtres.apply(lambda row: 'Irregular worktime' if quant['WorkTimeInSeconds'].iloc[row.name] else row['Reject'], axis = 1)\n",
    "\n",
    "#Reject everything with wrong codes\n",
    "amtres['Reject'] = amtres.apply(lambda row: 'Wrong code' if row['Answer.code'] != row['Input.ver_code'] else row['Reject'], axis = 1)\n",
    "\n",
    "#Reject obviously wrong data\n",
    "#Severity 0 but not 5 stars or source not 'correct'\n",
    "amtres['Reject'] = amtres.apply(lambda row: 'Wrong answer(s)' if row['severity'] == 0 and (row['Answer.stars'] != 5 or row['Answer.mistakeSource'] != 'correct') else row['Reject'], axis = 1)\n",
    "\n",
    "\n",
    "#Accept everything that wasn't rejected\n",
    "amtres['Approve'] = amtres.apply(lambda row: pd.isnull(row['Reject']), axis = 1)\n",
    "\n",
    "\n",
    "#Export file\n",
    "amtres.to_csv(\"amtres.csv\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Analyse rejected answers\n",
    "print(f\"Length: {len(amtres[amtres['Reject'].notnull()])}\")\n",
    "amtres[amtres['Reject'].notnull()].head(100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Analyse how many times were out of bounds\n",
    "print(f\"Length: {len(amtres[amtres['Reject'] == 'Irregular worktime'])}\")\n",
    "amtres[amtres['Reject'] == 'Irregular worktime'].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Length: {len(amtres)}\")\n",
    "amtres.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set global plot size\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total answers per rating\n",
    "amtres[amtres['Approve']][['Answer.stars', 'WorkerId']].groupby('Answer.stars').count().reset_index().rename(columns={'Answer.stars': 'rating', 'WorkerId': 'count'}).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answers per rating\n",
    "data = amtres[amtres['Approve']].copy()\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.plot(data[['Answer.stars', 'WorkerId', 'severity']].groupby(['Answer.stars', 'WorkerId']).count().groupby(['Answer.stars']).mean(),\n",
    "        label='mean',\n",
    "        linewidth=3.0)\n",
    "\n",
    "for n, g in data.groupby(['WorkerId']):\n",
    "    g = g.sort_values(['Answer.stars'])\n",
    "    plt.plot(g[['Answer.stars', 'WorkerId']].groupby('Answer.stars').count(),\n",
    "             marker='x',\n",
    "             label=g['WorkerId'].iloc[0],\n",
    "             linestyle='dashed'\n",
    "    )\n",
    "    \n",
    "plt.title('Answers per rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Number of votes')\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig('figures/answersPerRating.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Scatterplot of severity per rating\n",
    "data = amtres[amtres['Approve']].copy()\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "data['severity'] = data['severity'] + 1 #prevent missing data due to log(0)\n",
    "\n",
    "for n, g in data.groupby(['Input.method', 'Input.source_type']):\n",
    "    g = g.sort_values(['Answer.stars'])\n",
    "    plt.scatter(g['Answer.stars'], g['severity'],\n",
    "    marker=(\n",
    "        'x' if g['Input.method'].iloc[0] == 'random' else 'o'\n",
    "    ), label=(\n",
    "        (\n",
    "            'random node' if g['Input.source_type'].iloc[0] == 'node' else 'random relation'\n",
    "        ) if g['Input.method'].iloc[0] == 'random' else (\n",
    "            'dist node' if g['Input.source_type'].iloc[0] == 'node' else 'dist relation'\n",
    "        )\n",
    "    ))\n",
    "\n",
    "plt.title('Scatterplot of severity per rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Severity')\n",
    "plt.yscale('log')\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig('figures/scatterSeverityPerRating.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Unfiltered count per method and source\n",
    "amtres[['Answer.mistakeSource', 'WorkerId', 'Input.method']].groupby(['Input.method', 'Answer.mistakeSource']).count().reset_index().rename(columns={'WorkerId': 'count', 'Answer.mistakeSource': 'mistakeSource', 'Input.method': 'method'}).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count per method and source\n",
    "amtres[amtres['Approve']][['Answer.mistakeSource', 'WorkerId', 'Input.method']].groupby(['Input.method', 'Answer.mistakeSource']).count().reset_index().rename(columns={'WorkerId': 'count', 'Answer.mistakeSource': 'mistakeSource', 'Input.method': 'method'}).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count per rating and source\n",
    "amtres[amtres['Approve']][['Answer.stars', 'Answer.mistakeSource', 'WorkerId']].groupby(['Answer.stars', 'Answer.mistakeSource']).count().reset_index().rename(columns={'Answer.stars': 'rating', 'WorkerId': 'count', 'Answer.mistakeSource': 'mistakeSource'}).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source distribution by rating\n",
    "data = amtres[amtres['Approve']].copy()\n",
    "\n",
    "sources = list(data[['Answer.mistakeSource']].groupby(['Answer.mistakeSource']).first().index)\n",
    "\n",
    "data = data[['Answer.stars', 'Answer.mistakeSource', 'WorkerId']].groupby(['Answer.stars', 'Answer.mistakeSource']).count().reset_index().rename(columns={'WorkerId': 'count'})\n",
    "#.reset_index()\n",
    "#.rename(columns={'WorkerId': 'count'})\n",
    "\n",
    "for source in sources:\n",
    "    #print(data[data['Answer.mistakeSource'] == source])\n",
    "    #print(source)\n",
    "    data[source] = data[data['Answer.mistakeSource'] == source]['count']\n",
    "\n",
    "\n",
    "\n",
    "data = data[['Answer.stars', *sources]].groupby(['Answer.stars']).sum().reset_index()\n",
    "#data['correct'] = data.apply(lambda row: 0, axis=1) #TODO: only if source is missing completely\n",
    "\n",
    "data.plot( \n",
    "  x = 'Answer.stars',  \n",
    "  kind = 'barh',  \n",
    "  stacked = True,  \n",
    "  title = 'Percentage Stacked Bar Graph',  \n",
    "  mark_right = True) \n",
    "  \n",
    "df_total = data[\"artificial\"] + data[\"human\"] + data[\"correct\"] \n",
    "df_rel = data[data.columns[1:]].div(df_total, 0) * 100\n",
    "\n",
    "for n in df_rel: \n",
    "    for i, (cs, ab, pc) in enumerate(zip(data.iloc[:, 1:].cumsum(1)[n], data[n], df_rel[n])): \n",
    "        plt.text(cs - ab / 2, i, str(np.round(pc, 1)) + '%', va = 'center', ha = 'center', rotation = 70, fontsize = 8)\n",
    "        \n",
    "plt.savefig('figures/hbarSourceDistributionByRating.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count per severity and source\n",
    "amtres[amtres['Approve']][['severity', 'Answer.mistakeSource', 'WorkerId']].groupby(['severity', 'Answer.mistakeSource']).count().reset_index().rename(columns={'Answer.stars': 'rating', 'WorkerId': 'count', 'Answer.mistakeSource': 'mistakeSource'}).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Count of wrong/correct per worker\n",
    "amtCodeCheck = pd.DataFrame()\n",
    "\n",
    "amtCodeCheck[['WorkerId', 'incorrect']] = amtres[amtres['Answer.code'] != amtres['Input.ver_code']][['WorkerId', 'Answer.stars']].groupby('WorkerId').count().reset_index()\n",
    "amtCodeCheck[['WorkerId', 'correct']] = amtres[amtres['Answer.code'] == amtres['Input.ver_code']][['WorkerId', 'Answer.stars']].groupby('WorkerId').count().reset_index()\n",
    "\n",
    "amtCodeCheck.head(10) #.apply(pd.to_numeric, downcast='integer', errors='ignore', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unfiltered correlation overview\n",
    "amtres[['Answer.stars', 'WorkTimeInSeconds', 'severity', 'closeness']].rename(columns={'Answer.stars': 'rating', 'WorkerId': 'count', 'Answer.mistakeSource': 'mistakeSource'}).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation overview\n",
    "amtres[amtres['Approve']][['Answer.stars', 'WorkTimeInSeconds', 'severity', 'closeness']].rename(columns={'Answer.stars': 'rating', 'WorkerId': 'count', 'Answer.mistakeSource': 'mistakeSource'}).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Coorelation per method per type \n",
    "amtres[amtres['Approve']][['Answer.stars', 'WorkTimeInSeconds', 'severity', 'closeness', 'Input.method', 'Input.source_type']].rename(columns={'Answer.stars': 'rating', 'Input.method': 'method', 'Input.source_type': 'elementType'}).groupby(['method', 'elementType']).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot of severity per rating\n",
    "data = amtres[amtres['Approve']].copy()\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "\n",
    "dat = []\n",
    "lab = []\n",
    "for n, g in data.groupby(['Answer.stars']):\n",
    "    dat += [g['severity']]\n",
    "    lab += [g['Answer.stars'].iloc[0]]\n",
    "\n",
    "plt.boxplot(dat, labels=lab)\n",
    "\n",
    "\n",
    "plt.title('Boxplot of severity per rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Severity')\n",
    "plt.yscale('log')\n",
    "#ax.legend()\n",
    "\n",
    "plt.savefig('figures/boxplotSeverityPerRating.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check dictionary quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare all dictionaries\n",
    "dicts = {\"gn\": gn, \"gw1\": gw1, \"gw3\": gw3}\n",
    "\n",
    "for i in dicts:\n",
    "    dic = dicts[i]\n",
    "    \n",
    "    #Minus means arrow from R to L\n",
    "    hasCapital = dic[\"tokyo\"] - dic[\"japan\"]\n",
    "    isCapitalOf = dic[\"japan\"] - dic[\"tokyo\"]\n",
    "    \n",
    "    #Calculate results of relation\n",
    "    est_france = dic[\"paris\"] + isCapitalOf\n",
    "    est_paris = dic[\"france\"] + hasCapital\n",
    "    \n",
    "    print(f\"Dictionary: {i}\")\n",
    "    print(f\"est_France to France: {dic.cosine_similarities(est_france, [dic['france']])}\")\n",
    "    print(f\"est_Paris to Paris: {dic.cosine_similarities(est_paris, [dic['paris']])}\")\n",
    "    print(f\"est_France to Paris: {dic.cosine_similarities(est_france, [dic['paris']])}\")\n",
    "    print(f\"est_Paris to France: {dic.cosine_similarities(est_paris, [dic['france']])}\")\n",
    "    print(f\"Paris to France: {dic.cosine_similarities(dic['paris'], [dic['france']])}\")\n",
    "    print(f\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Similarity: {wv.cosine_similarities(wv['white'], [wv['black']])[0]}\\n\")\n",
    "\n",
    "print(\"Inverted node 'white':\")\n",
    "for d in wv.similar_by_vector(vector = -1*wv['white']):\n",
    "    print(f\"Distance of '{d[0]}' to !white: {d[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}